{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0224de9-0790-4049-a8c4-909217215ae9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "video without yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcea09-f734-4ac5-b335-e32d46eb8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, uuid, time, requests\n",
    "\n",
    "# === CONFIG ===\n",
    "COMFY = \"http://192.168.27.13:23476\"   # your ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "INPUT_IMAGE = \"input.png\"               # input image path\n",
    "OUTPUT_DIR = \"comfy_output\"\n",
    "GDINO_PROMPT = \"clothes\"\n",
    "GDINO_THRESHOLD = 0.35\n",
    "# ===============\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def upload_image_to_comfy(local_path, server=COMFY, folder_type=\"input\"):\n",
    "    \"\"\"Upload image to ComfyUI input folder.\"\"\"\n",
    "    dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "\n",
    "def patch_nodes(prompt_dict, image_name, new_prompt=None, new_threshold=None):\n",
    "    \"\"\"Patch LoadImage and GroundingDino nodes.\"\"\"\n",
    "    for node in prompt_dict.values():\n",
    "        cls = node.get(\"class_type\", \"\").lower()\n",
    "        if cls == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = image_name\n",
    "        if cls.startswith(\"groundingdinosamsegment\"):\n",
    "            if new_prompt: node[\"inputs\"][\"prompt\"] = new_prompt\n",
    "            if new_threshold: node[\"inputs\"][\"threshold\"] = new_threshold\n",
    "\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def download_output_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", save_dir=OUTPUT_DIR):\n",
    "    \"\"\"Download result image from ComfyUI output folder.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    out_path = os.path.join(save_dir, filename)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_sam_on_image(image_path):\n",
    "    \"\"\"Run SAM workflow via ComfyUI and return output image path.\"\"\"\n",
    "    # 1Ô∏è‚É£ Upload\n",
    "    uploaded = upload_image_to_comfy(image_path)\n",
    "\n",
    "    # 2Ô∏è‚É£ Load and patch workflow JSON\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_nodes(prompt, uploaded, new_prompt=GDINO_PROMPT, new_threshold=GDINO_THRESHOLD)\n",
    "\n",
    "    # 3Ô∏è‚É£ Send to ComfyUI\n",
    "    prompt_id = queue_prompt(prompt)\n",
    "\n",
    "    # 4Ô∏è‚É£ Poll for output\n",
    "    print(\"[INFO] Waiting for ComfyUI output...\")\n",
    "    seg_path = None\n",
    "    deadline = time.time() + 600\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "                    seg_path = download_output_image(fn, folder_type=typ, subfolder=sub)\n",
    "                    break\n",
    "        if seg_path:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    if not seg_path:\n",
    "        raise RuntimeError(\"No output image received from ComfyUI\")\n",
    "\n",
    "    print(f\"[‚úÖ] SAM output saved to: {seg_path}\")\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ==== RUN ====\n",
    "if __name__ == \"__main__\":\n",
    "    output_image = run_sam_on_image(INPUT_IMAGE)\n",
    "    print(\"Output:\", output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252aa25d-175a-4357-abe6-f2c8fab0feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Using: NVIDIA GeForce RTX 5060 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/deoldify/fastai/imports/core.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/opt/deoldify/fastai/data_block.py:451: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\n",
      "  warn(\"Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\")\n",
      "/opt/deoldify/fastai/data_block.py:453: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  warn(\"\"\"Your validation set is empty. If this is by design, use `split_none()`\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /workspace/models/torch/hub/checkpoints/resnet34-b627a593.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83.3M/83.3M [00:01<00:00, 44.6MB/s]\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL functools.partial was not an allowed global by default. Please use `torch.serialization.add_safe_globals([functools.partial])` or the `torch.serialization.safe_globals([functools.partial])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# =================\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# ---- Setup DeOldify ----\u001b[39;00m\n\u001b[32m     81\u001b[39m device.set(device=DeviceId.GPU0)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m colorizer = \u001b[43mget_image_colorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43martistic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# ---- Setup YOLO ----\u001b[39;00m\n\u001b[32m     85\u001b[39m device_str = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/deoldify/deoldify/visualize.py:457\u001b[39m, in \u001b[36mget_image_colorizer\u001b[39m\u001b[34m(root_folder, render_factor, artistic)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_image_colorizer\u001b[39m(\n\u001b[32m    454\u001b[39m     root_folder: Path = Path(\u001b[33m'\u001b[39m\u001b[33m./\u001b[39m\u001b[33m'\u001b[39m), render_factor: \u001b[38;5;28mint\u001b[39m = \u001b[32m35\u001b[39m, artistic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    455\u001b[39m ) -> ModelImageVisualizer:\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artistic:\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_artistic_image_colorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrender_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    459\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m get_stable_image_colorizer(root_folder=root_folder, render_factor=render_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/deoldify/deoldify/visualize.py:480\u001b[39m, in \u001b[36mget_artistic_image_colorizer\u001b[39m\u001b[34m(root_folder, weights_name, results_dir, render_factor)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_artistic_image_colorizer\u001b[39m(\n\u001b[32m    475\u001b[39m     root_folder: Path = Path(\u001b[33m'\u001b[39m\u001b[33m./\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    476\u001b[39m     weights_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mColorizeArtistic_gen\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    477\u001b[39m     results_dir=\u001b[33m'\u001b[39m\u001b[33mresult_images\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    478\u001b[39m     render_factor: \u001b[38;5;28mint\u001b[39m = \u001b[32m35\u001b[39m\n\u001b[32m    479\u001b[39m ) -> ModelImageVisualizer:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     learn = \u001b[43mgen_inference_deep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     filtr = MasterFilter([ColorizerFilter(learn=learn)], render_factor=render_factor)\n\u001b[32m    482\u001b[39m     vis = ModelImageVisualizer(filtr, results_dir=results_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/deoldify/deoldify/generators.py:92\u001b[39m, in \u001b[36mgen_inference_deep\u001b[39m\u001b[34m(root_folder, weights_name, arch, nf_factor)\u001b[39m\n\u001b[32m     88\u001b[39m learn = gen_learner_deep(\n\u001b[32m     89\u001b[39m     data=data, gen_loss=F.l1_loss, arch=arch, nf_factor=nf_factor\n\u001b[32m     90\u001b[39m )\n\u001b[32m     91\u001b[39m learn.path = root_folder\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43mlearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m learn.model.eval()\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m learn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/deoldify/fastai/basic_train.py:267\u001b[39m, in \u001b[36mLearner.load\u001b[39m\u001b[34m(self, file, device, strict, with_opt, purge, remove_module)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, file:PathLikeOrBinaryStream=\u001b[38;5;28;01mNone\u001b[39;00m, device:torch.device=\u001b[38;5;28;01mNone\u001b[39;00m, strict:\u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    265\u001b[39m          with_opt:\u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, purge:\u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_module:\u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    266\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLoad model and optimizer state (if `with_opt`) `file` from `self.model_dir` using `device`. `file` can be file-like (file or buffer)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m purge: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpurge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclear_opt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mifnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: device = \u001b[38;5;28mself\u001b[39m.data.device\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m): device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/deoldify/fastai/basic_train.py:322\u001b[39m, in \u001b[36mLearner.purge\u001b[39m\u001b[34m(self, clear_opt)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attrs_del: \u001b[38;5;28mdelattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, a)\n\u001b[32m    321\u001b[39m gc.collect()\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m state = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m os.remove(tmp_file)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attrs_pkl: \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, state[a])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/torch/serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL functools.partial was not an allowed global by default. Please use `torch.serialization.add_safe_globals([functools.partial])` or the `torch.serialization.safe_globals([functools.partial])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "GDINO_PROMPT = \"clothes\" # grounding dino prompt\n",
    "GDINO_THRESHOLD = 0.35   # grounding dino threshold\n",
    "# =================\n",
    "ENLARGE_PERCENT = 0.2 \n",
    "USE_REGION_AB_AVERAGE = True   # üîπ new parameter to toggle region-averaged colors\n",
    "def apply_region_color_transfer(f_in, f_de, mask):\n",
    "    if not USE_REGION_AB_AVERAGE:\n",
    "        # default pixel-wise transfer\n",
    "        out = f_in.copy()\n",
    "        out[mask > 127] = f_de[mask > 127]\n",
    "        return out\n",
    "\n",
    "    # Convert both to LAB\n",
    "    lab_in = cv2.cvtColor(f_in, cv2.COLOR_BGR2LAB)\n",
    "    lab_de = cv2.cvtColor(f_de, cv2.COLOR_BGR2LAB)\n",
    "    out_lab = lab_in.copy()\n",
    "\n",
    "    # Find connected regions\n",
    "    num_labels, labels = cv2.connectedComponents((mask > 127).astype(np.uint8))\n",
    "\n",
    "    for lbl in range(1, num_labels):  # skip background\n",
    "        region_mask = (labels == lbl)\n",
    "\n",
    "        # Extract A/B values from DeOldify\n",
    "        A_vals = lab_de[...,1][region_mask]\n",
    "        B_vals = lab_de[...,2][region_mask]\n",
    "        if A_vals.size == 0: continue\n",
    "\n",
    "        meanA, meanB = int(np.mean(A_vals)), int(np.mean(B_vals))\n",
    "\n",
    "        # Replace only A/B, keep original L\n",
    "        out_lab[...,1][region_mask] = meanA\n",
    "        out_lab[...,2][region_mask] = meanB\n",
    "\n",
    "    # Convert back to BGR\n",
    "    fused_bgr = cv2.cvtColor(out_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Apply only to mask area\n",
    "    out = f_in.copy()\n",
    "    out[mask > 127] = fused_bgr[mask > 127]\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def patch_groundingdino_node(prompt_dict, new_prompt=None, new_threshold=None):\n",
    "    \"\"\"Patch GroundingDinoSAMSegment node with new prompt/threshold values.\"\"\"\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\", \"\").lower().startswith(\"groundingdinosamsegment\"):\n",
    "            if new_prompt is not None:\n",
    "                node[\"inputs\"][\"prompt\"] = new_prompt\n",
    "            if new_threshold is not None:\n",
    "                node[\"inputs\"][\"threshold\"] = new_threshold\n",
    "            return True\n",
    "    return False\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    return to_path\n",
    "\n",
    "\n",
    "def run_sam_on_frame(frame_path, comfy_server=COMFY):\n",
    "    \"\"\"Send one frame through ComfyUI workflow and return saved ComfyUI_* path.\"\"\"\n",
    "    uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "    # Load workflow JSON\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # Patch LoadImage node\n",
    "    if not patch_loadimage_node(prompt, uploaded):\n",
    "        raise RuntimeError(\"Could not patch LoadImage node in workflow JSON.\")\n",
    "\n",
    "    patch_groundingdino_node(prompt, new_prompt=GDINO_PROMPT, new_threshold=GDINO_THRESHOLD)\n",
    "    # Queue\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 600  # up to 10 min per frame\n",
    "    seg_path = None\n",
    "\n",
    "    # Poll history until output is ready\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "                    # Save as ComfyUI_<originalname>.png in same folder\n",
    "                    base = os.path.splitext(os.path.basename(frame_path))[0]\n",
    "                    save_dir = os.path.dirname(frame_path)\n",
    "                    out_path = os.path.join(save_dir, f\"ComfyUI_{base}.png\")\n",
    "                    seg_path = download_image(fn, server=comfy_server,\n",
    "                                              subfolder=sub, folder_type=typ,\n",
    "                                              to_path=out_path)\n",
    "                    break\n",
    "        if seg_path:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if not seg_path:\n",
    "        raise RuntimeError(f\"No outputs from ComfyUI for {frame_path}\")\n",
    "\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM (extract ‚Üí ComfyUI per frame ‚Üí video) ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM video: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    # --- Step 1: Extract all frames first ---\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Extracting frames\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_file = os.path.join(sam_frames_dir, f\"frame_{idx:06d}.png\")\n",
    "            if not os.path.exists(frame_file):\n",
    "                cv2.imwrite(frame_file, frame_bgr)\n",
    "            idx += 1\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "\n",
    "    # --- Step 2: Run ComfyUI on each frame ---\n",
    "    frame_files = sorted([f for f in os.listdir(sam_frames_dir) if f.startswith(\"frame_\")])\n",
    "    for fname in tqdm(frame_files, desc=\"Processing with ComfyUI\", unit=\"frame\"):\n",
    "        frame_file = os.path.join(sam_frames_dir, fname)\n",
    "        out_file = os.path.join(sam_frames_dir, f\"ComfyUI_{fname}\")\n",
    "        if os.path.exists(out_file):\n",
    "            continue\n",
    "        try:\n",
    "            run_sam_on_frame(frame_file, comfy_server=COMFY)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è SAM failed on {fname}: {e}\")\n",
    "\n",
    "    # --- Step 3: Collect only ComfyUI_* frames ---\n",
    "    sam_files = sorted([f for f in os.listdir(sam_frames_dir) if f.startswith(\"ComfyUI_\")])\n",
    "\n",
    "    # --- Step 4: Combine into video ---\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for seg_file in tqdm(sam_files, desc=\"Building SAM video\", unit=\"frame\"):\n",
    "        img = cv2.imread(os.path.join(sam_frames_dir, seg_file))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img_resized = cv2.resize(img, (width, height))\n",
    "        writer.write(img_resized)\n",
    "\n",
    "    writer.release()\n",
    "    print(f\"[INFO] SAM video saved: {sam_path}\")\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "\n",
    "def run_deoldify(input_path, out_dir, name, force_bw=True):\n",
    "    \"\"\"\n",
    "    Run DeOldify on a video.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to input video.\n",
    "        out_dir (str): Output directory.\n",
    "        name (str): Base name for output file.\n",
    "        force_bw (bool): If True, convert input frames to grayscale before DeOldify.\n",
    "                         If False, feed original color frames (default = True).\n",
    "    \"\"\"\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if force_bw:\n",
    "                # üîπ Convert to grayscale first, then back to 3-channel RGB\n",
    "                frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "                frame_rgb = cv2.cvtColor(frame_gray, cv2.COLOR_GRAY2RGB)\n",
    "            else:\n",
    "                # üîπ Use original color frame\n",
    "                frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM (with frame folder + resume support) ----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)      # original frames\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)   # deoldify video\n",
    "    cap_sam = cv2.VideoCapture(sam_path)          # sam masks\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion: base is ORIGINAL frame\n",
    "            fusion_frame = frame_in.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    \"\"\"\n",
    "    Fusion step with YOLO bounding box enlargement using global ENLARGE_PERCENT.\n",
    "    \"\"\"\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # --- SAM mask ---\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # --- YOLO mask with global enlargement ---\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # expand box by percentage\n",
    "                dx = int((x2 - x1) * ENLARGE_PERCENT)\n",
    "                dy = int((y2 - y1) * ENLARGE_PERCENT)\n",
    "\n",
    "                x1 = max(0, x1 - dx)\n",
    "                y1 = max(0, y1 - dy)\n",
    "                x2 = min(width,  x2 + dx)\n",
    "                y2 = min(height, y2 + dy)\n",
    "\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # --- Intersection ---\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # # --- Fusion ---\n",
    "            # fusion_frame = frame_in.copy()\n",
    "            # fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            # writer.write(fusion_frame)\n",
    "\n",
    "            # --- Fusion with optional region-averaged A/B ---\n",
    "            fusion_frame = apply_region_color_transfer(frame_in, frame_deold, intersect)\n",
    "            writer.write(fusion_frame)\n",
    "\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "def process_video_cached(input_path):\n",
    "    \"\"\"\n",
    "    Cached wrapper around process_video().\n",
    "    Returns only the final fusion video path.\n",
    "    \"\"\"\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    final_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"[CACHE] Final output exists: {final_path}\")\n",
    "        return final_path\n",
    "\n",
    "    outputs = process_video(input_path)\n",
    "    return outputs[\"final\"]\n",
    "\n",
    "\n",
    "# # ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "#     outputs = process_video(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k, v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f9b52-87ff-4ebb-badc-6b751f0f75f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image without yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4438b42-d3b3-41c8-b12d-1771ee9dbcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outputs written to outputs/first_frame\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/first_frame/first_frame_yolo.png\n",
      " - deoldify: outputs/first_frame/first_frame_deoldify.png\n",
      " - sam: outputs/first_frame/first_frame_sam.png\n",
      " - final: outputs/first_frame/first_frame_final.png\n"
     ]
    }
   ],
   "source": [
    "GDINO_PROMPT = \"clothes\" # grounding dino prompt\n",
    "GDINO_THRESHOLD = 0.30   # grounding dino threshold\n",
    "\n",
    "USE_REGION_AB_AVERAGE = True     # Use DeOldify‚Äôs averaged A/B if True\n",
    "USE_CUSTOM_AB = True            # If True, override with fixed values\n",
    "CUSTOM_A = 128                # Example fixed A channel value (0‚Äì255)\n",
    "CUSTOM_B = 115                  # Example fixed B channel value (0‚Äì255)\n",
    "\n",
    "\n",
    "def apply_region_color_transfer(f_in, f_de, mask):\n",
    "    if not (USE_REGION_AB_AVERAGE or USE_CUSTOM_AB):\n",
    "        # default per-pixel DeOldify transfer\n",
    "        out = f_in.copy()\n",
    "        out[mask > 127] = f_de[mask > 127]\n",
    "        return out\n",
    "\n",
    "    lab_in = cv2.cvtColor(f_in, cv2.COLOR_BGR2LAB)\n",
    "    lab_de = cv2.cvtColor(f_de, cv2.COLOR_BGR2LAB)\n",
    "    out_lab = lab_in.copy()\n",
    "\n",
    "    num_labels, labels = cv2.connectedComponents((mask > 127).astype(np.uint8))\n",
    "    for lbl in range(1, num_labels):\n",
    "        region_mask = (labels == lbl)\n",
    "\n",
    "        if USE_CUSTOM_AB:\n",
    "            # üîπ Force custom values for all regions\n",
    "            meanA, meanB = CUSTOM_A, CUSTOM_B\n",
    "        else:\n",
    "            # üîπ Average DeOldify‚Äôs A/B values\n",
    "            A_vals = lab_de[...,1][region_mask]\n",
    "            B_vals = lab_de[...,2][region_mask]\n",
    "            if A_vals.size == 0: \n",
    "                continue\n",
    "            meanA, meanB = int(np.mean(A_vals)), int(np.mean(B_vals))\n",
    "\n",
    "        out_lab[...,1][region_mask] = meanA\n",
    "        out_lab[...,2][region_mask] = meanB\n",
    "\n",
    "    fused_bgr = cv2.cvtColor(out_lab, cv2.COLOR_LAB2BGR)\n",
    "    out = f_in.copy()\n",
    "    out[mask > 127] = fused_bgr[mask > 127]\n",
    "    return out\n",
    "\n",
    "\n",
    "def process_image(input_image):\n",
    "    folder, fname = os.path.split(input_image)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ---- Load frame ----\n",
    "    frame_bgr = cv2.imread(input_image)\n",
    "    if frame_bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not load image {input_image}\")\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "\n",
    "    # ---- Stage 1: YOLO ----\n",
    "    yolo_results = yolo_model.predict(frame_bgr, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "    yolo_frame = yolo_results[0].plot()\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.png\")\n",
    "    cv2.imwrite(yolo_path, yolo_frame)\n",
    "\n",
    "    results_dict = {\n",
    "        \"boxes\": yolo_results[0].boxes.xyxy.cpu().numpy(),\n",
    "        \"conf\": yolo_results[0].boxes.conf.cpu().numpy(),\n",
    "        \"cls\": yolo_results[0].boxes.cls.cpu().numpy()\n",
    "    }\n",
    "    with open(os.path.join(out_dir, f\"{name}_yolo_results.pkl\"), \"wb\") as f:\n",
    "        pickle.dump([results_dict], f)\n",
    "\n",
    "    # ---- Stage 2: DeOldify ----\n",
    "    f_gray = cv2.cvtColor(frame_rgb, cv2.COLOR_BGR2GRAY)\n",
    "    f_gray3 = cv2.cvtColor(f_gray, cv2.COLOR_GRAY2RGB)\n",
    "    deold = deoldify_inference(f_gray3)\n",
    "    deold_bgr = cv2.cvtColor(deold, cv2.COLOR_RGB2BGR)\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.png\")\n",
    "    cv2.imwrite(deoldify_path, deold_bgr)\n",
    "\n",
    "    # ---- Stage 3: SAM (ComfyUI workflow on full image) ----\n",
    "    try:\n",
    "        seg_path = run_sam_on_frame(input_image, comfy_server=COMFY)\n",
    "        seg_img = cv2.imread(seg_path)\n",
    "        sam_gray = cv2.cvtColor(seg_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, sam_mask = cv2.threshold(sam_gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è SAM failed on image: {e}\")\n",
    "        sam_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.png\")\n",
    "    cv2.imwrite(sam_path, sam_mask)\n",
    "\n",
    "    # ---- Stage 4: Fusion (YOLO ‚à© SAM mask) ----\n",
    "    yolo_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    for box, conf, cls in zip(results_dict[\"boxes\"], results_dict[\"conf\"], results_dict[\"cls\"]):\n",
    "        if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "    intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "    mask_bool = intersect > 127\n",
    "\n",
    "    # fusion_frame = frame_bgr.copy()\n",
    "    # fusion_frame[mask_bool] = deold_bgr[mask_bool]\n",
    "\n",
    "    fusion_frame = apply_region_color_transfer(frame_bgr, deold_bgr, intersect)\n",
    "\n",
    "\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.png\")\n",
    "    cv2.imwrite(fusion_path, fusion_frame)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_image =\"input_videos/first_frame.jpg\"\n",
    "    outputs = process_image(input_image)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23caad-fdc9-415e-9f0f-a8a3d2b89ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image (BGR format by default in OpenCV)\n",
    "img_bgr = cv2.imread(\"input_videos/input_100.jpg\")\n",
    "\n",
    "# Convert BGR ‚Üí Grayscale\n",
    "img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Save the grayscale image\n",
    "cv2.imwrite(\"input_videos/input_100_g.jpg\", img_gray)\n",
    "\n",
    "# (Optional) Display images\n",
    "cv2.imshow(\"Original BGR\", img_bgr)\n",
    "cv2.imshow(\"Grayscale\", img_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1436b-80b2-4670-9c83-1230732cc828",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "video with yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e647265b-7647-4810-bfc3-43f55ed693b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Using: NVIDIA GeForce RTX 4060 Ti\n",
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "\n",
    "# ---- NEW CONFIG ----\n",
    "BBOX_ENLARGE = 0.2       # enlarge bbox by 20%\n",
    "TOP_K_BBOX = 6           # number of top boxes per frame to run SAM\n",
    "GDINO_PROMPT = \"clothes\" # grounding dino prompt\n",
    "GDINO_THRESHOLD = 0.35   # grounding dino threshold\n",
    "# =================\n",
    "\n",
    "\n",
    "def patch_groundingdino_node(prompt_dict, new_prompt=None, new_threshold=None):\n",
    "    \"\"\"Patch GroundingDinoSAMSegment node with new prompt/threshold values.\"\"\"\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\", \"\").lower().startswith(\"groundingdinosamsegment\"):\n",
    "            if new_prompt is not None:\n",
    "                node[\"inputs\"][\"prompt\"] = new_prompt\n",
    "            if new_threshold is not None:\n",
    "                node[\"inputs\"][\"threshold\"] = new_threshold\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    return to_path\n",
    "\n",
    "\n",
    "def run_sam_on_frame(frame_path, comfy_server=COMFY):\n",
    "    \"\"\"Send one frame (crop) through ComfyUI workflow and return saved mask path.\"\"\"\n",
    "    uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # Patch LoadImage node\n",
    "    if not patch_loadimage_node(prompt, uploaded):\n",
    "        raise RuntimeError(\"Could not patch LoadImage node in workflow JSON.\")\n",
    "\n",
    "    # üîπ Patch GroundingDino node with dynamic prompt & threshold\n",
    "    patch_groundingdino_node(prompt, new_prompt=GDINO_PROMPT, new_threshold=GDINO_THRESHOLD)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 600\n",
    "    seg_path = None\n",
    "\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "                    base = os.path.splitext(os.path.basename(frame_path))[0]\n",
    "                    save_dir = os.path.dirname(frame_path)\n",
    "                    out_path = os.path.join(save_dir, f\"ComfyUI_{base}.png\")\n",
    "                    seg_path = download_image(fn, server=comfy_server,\n",
    "                                              subfolder=sub, folder_type=typ,\n",
    "                                              to_path=out_path)\n",
    "                    break\n",
    "        if seg_path:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if not seg_path:\n",
    "        raise RuntimeError(f\"No outputs from ComfyUI for {frame_path}\")\n",
    "\n",
    "    return seg_path\n",
    "\n",
    "# ---- Stage 3: SAM with YOLO BBoxes ----\n",
    "def run_sam(input_path, out_dir, name, yolo_results):\n",
    "    sam_frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM video: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"SAM with YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            results = yolo_results[frame_idx]\n",
    "            boxes, confs, clses = results[\"boxes\"], results[\"conf\"], results[\"cls\"]\n",
    "\n",
    "            order = np.argsort(confs)[::-1][:TOP_K_BBOX]\n",
    "            masks_for_frame = []\n",
    "\n",
    "            for i in order:\n",
    "                if int(clses[i]) != 0 or confs[i] < CONF_THRESHOLD:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, boxes[i])\n",
    "\n",
    "                # enlarge box\n",
    "                bw = x2 - x1\n",
    "                bh = y2 - y1\n",
    "                x1 = max(0, int(x1 - BBOX_ENLARGE * bw))\n",
    "                y1 = max(0, int(y1 - BBOX_ENLARGE * bh))\n",
    "                x2 = min(width, int(x2 + BBOX_ENLARGE * bw))\n",
    "                y2 = min(height, int(y2 + BBOX_ENLARGE * bh))\n",
    "\n",
    "                crop = frame_bgr[y1:y2, x1:x2]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "\n",
    "                crop_path = os.path.join(sam_frames_dir, f\"frame_{frame_idx:06d}_box{i}.png\")\n",
    "                cv2.imwrite(crop_path, crop)\n",
    "\n",
    "                try:\n",
    "                    seg_path = run_sam_on_frame(crop_path, comfy_server=COMFY)\n",
    "                    seg_img = cv2.imread(seg_path)\n",
    "                    seg_resized = cv2.resize(seg_img, (x2 - x1, y2 - y1))\n",
    "                    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "                    mask[y1:y2, x1:x2] = cv2.cvtColor(seg_resized, cv2.COLOR_BGR2GRAY)\n",
    "                    masks_for_frame.append(mask)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è SAM failed on frame {frame_idx}, box {i}: {e}\")\n",
    "                finally:\n",
    "                    # cleanup crop + box-level SAM output\n",
    "                    if os.path.exists(crop_path):\n",
    "                        os.remove(crop_path)\n",
    "                    box_seg = crop_path.replace(\".png\", \"\").replace(\"frame_\", \"ComfyUI_frame_\") + \".png\"\n",
    "                    if os.path.exists(box_seg):\n",
    "                        os.remove(box_seg)\n",
    "\n",
    "            # always save a mask (blank if no detections)\n",
    "            if masks_for_frame:\n",
    "                final_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "                for m in masks_for_frame:\n",
    "                    final_mask = cv2.bitwise_or(final_mask, m)\n",
    "            else:\n",
    "                final_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "            out_path = os.path.join(sam_frames_dir, f\"ComfyUI_frame_{frame_idx:06d}.png\")\n",
    "            cv2.imwrite(out_path, final_mask)\n",
    "\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "\n",
    "    # build video ONLY from final per-frame masks\n",
    "    sam_files = sorted([\n",
    "        f for f in os.listdir(sam_frames_dir)\n",
    "        if f.startswith(\"ComfyUI_frame_\") and \"_box\" not in f\n",
    "    ])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "    for seg_file in tqdm(sam_files, desc=\"Building SAM video\", unit=\"frame\"):\n",
    "        img = cv2.imread(os.path.join(sam_frames_dir, seg_file))\n",
    "        writer.write(cv2.resize(img, (width, height)))\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # üîπ Convert frame to grayscale, then back to 3-channel RGB\n",
    "            frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "            frame_rgb = cv2.cvtColor(frame_gray, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "            mask_bool = sam_mask > 127\n",
    "\n",
    "            fusion_frame = frame_in.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name, yolo_results)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def process_video_cached(input_path):\n",
    "    \"\"\"\n",
    "    Cached wrapper around process_video().\n",
    "    Returns only the final fusion video path.\n",
    "    \"\"\"\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    final_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"[CACHE] Final output exists: {final_path}\")\n",
    "        return final_path\n",
    "\n",
    "    outputs = process_video(input_path)\n",
    "    return outputs[\"final\"]\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "#     outputs = process_video(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k, v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3e7475-401d-413e-858e-3225260ad589",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4134125518.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimage with yolo clipping\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "image with yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cb6f03-9449-4981-bc97-5bcc5444408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outputs written to outputs/12th_from_last_frame\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/12th_from_last_frame/12th_from_last_frame_yolo.png\n",
      " - deoldify: outputs/12th_from_last_frame/12th_from_last_frame_deoldify.png\n",
      " - sam: outputs/12th_from_last_frame/12th_from_last_frame_sam.png\n",
      " - final: outputs/12th_from_last_frame/12th_from_last_frame_final.png\n"
     ]
    }
   ],
   "source": [
    "GDINO_PROMPT = \"arms\" # grounding dino prompt\n",
    "GDINO_THRESHOLD = 0.27   # grounding dino threshold\n",
    "\n",
    "\n",
    "def process_image(input_image):\n",
    "    folder, fname = os.path.split(input_image)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ---- Load frame ----\n",
    "    frame_bgr = cv2.imread(input_image)\n",
    "    if frame_bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not load image {input_image}\")\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "\n",
    "    # ---- Stage 1: YOLO ----\n",
    "    yolo_results = yolo_model.predict(frame_bgr, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "    yolo_frame = yolo_results[0].plot()\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.png\")\n",
    "    cv2.imwrite(yolo_path, yolo_frame)\n",
    "\n",
    "    results_dict = {\n",
    "        \"boxes\": yolo_results[0].boxes.xyxy.cpu().numpy(),\n",
    "        \"conf\": yolo_results[0].boxes.conf.cpu().numpy(),\n",
    "        \"cls\": yolo_results[0].boxes.cls.cpu().numpy()\n",
    "    }\n",
    "    with open(os.path.join(out_dir, f\"{name}_yolo_results.pkl\"), \"wb\") as f:\n",
    "        pickle.dump([results_dict], f)\n",
    "\n",
    "    # ---- Stage 2: DeOldify ----\n",
    "    f_gray = cv2.cvtColor(frame_rgb, cv2.COLOR_BGR2GRAY)\n",
    "    f_gray3 = cv2.cvtColor(f_gray, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    deold = deoldify_inference(f_gray3)\n",
    "    deold_bgr = cv2.cvtColor(deold, cv2.COLOR_RGB2BGR)\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.png\")\n",
    "    cv2.imwrite(deoldify_path, deold_bgr)\n",
    "\n",
    "    # ---- Stage 3: SAM ----\n",
    "    sam_frames_dir = os.path.join(out_dir, f\"{name}_sam\")\n",
    "    os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "\n",
    "    boxes, confs, clses = results_dict[\"boxes\"], results_dict[\"conf\"], results_dict[\"cls\"]\n",
    "    order = np.argsort(confs)[::-1][:TOP_K_BBOX]\n",
    "    masks_for_frame = []\n",
    "\n",
    "    for i in order:\n",
    "        if int(clses[i]) != 0 or confs[i] < CONF_THRESHOLD:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, boxes[i])\n",
    "        bw, bh = x2 - x1, y2 - y1\n",
    "        x1 = max(0, int(x1 - BBOX_ENLARGE * bw))\n",
    "        y1 = max(0, int(y1 - BBOX_ENLARGE * bh))\n",
    "        x2 = min(w, int(x2 + BBOX_ENLARGE * bw))\n",
    "        y2 = min(h, int(y2 + BBOX_ENLARGE * bh))\n",
    "\n",
    "        crop = frame_bgr[y1:y2, x1:x2]\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        crop_path = os.path.join(sam_frames_dir, f\"{name}_box{i}.png\")\n",
    "        cv2.imwrite(crop_path, crop)\n",
    "\n",
    "        try:\n",
    "            seg_path = run_sam_on_frame(crop_path, comfy_server=COMFY)\n",
    "            seg_img = cv2.imread(seg_path)\n",
    "\n",
    "            \n",
    "            # seg_resized = cv2.resize(seg_img, (x2 - x1, y2 - y1))\n",
    "            # mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            # mask[y1:y2, x1:x2] = cv2.cvtColor(seg_resized, cv2.COLOR_BGR2GRAY)\n",
    "            # masks_for_frame.append(mask)\n",
    "            seg_resized = cv2.resize(seg_img, (x2 - x1, y2 - y1))\n",
    "            seg_gray = cv2.cvtColor(seg_resized, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # üîπ Add this line to force binary\n",
    "            _, seg_bin = cv2.threshold(seg_gray, 1, 255, cv2.THRESH_BINARY)\n",
    "            \n",
    "            mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            mask[y1:y2, x1:x2] = seg_bin\n",
    "            masks_for_frame.append(mask)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"SAM failed on box {i}: {e}\")\n",
    "\n",
    "    # Save final mask (blank if none)\n",
    "    if masks_for_frame:\n",
    "        final_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        for m in masks_for_frame:\n",
    "            final_mask = cv2.bitwise_or(final_mask, m)\n",
    "    else:\n",
    "        final_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.png\")\n",
    "    cv2.imwrite(sam_path, final_mask)\n",
    "\n",
    "    # ---- Stage 4: Fusion (exact same logic as run_fusion) ----\n",
    "    gray = final_mask.copy()\n",
    "    _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    mask_bool = sam_mask > 127\n",
    "\n",
    "    fusion_frame = frame_bgr.copy()\n",
    "    fusion_frame[mask_bool] = deold_bgr[mask_bool]\n",
    "\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.png\")\n",
    "    cv2.imwrite(fusion_path, fusion_frame)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_image = r\"input_videos/12th_from_last_frame.jpg\"\n",
    "    outputs = process_image(input_image)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cb7b3-bca5-4b1e-aa70-a31949437d06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image with yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa4364a3-5528-4370-a0a4-509e13576aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDINO_PROMPT = \"clothes\" # grounding dino prompt\n",
    "# GDINO_THRESHOLD = 0.30   # grounding dino threshold\n",
    "\n",
    "\n",
    "# def process_image(input_image):\n",
    "#     folder, fname = os.path.split(input_image)\n",
    "#     name, _ = os.path.splitext(fname)\n",
    "#     out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     # ---- Load frame ----\n",
    "#     frame_bgr = cv2.imread(input_image)\n",
    "#     if frame_bgr is None:\n",
    "#         raise FileNotFoundError(f\"Could not load image {input_image}\")\n",
    "#     frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "#     h, w = frame_bgr.shape[:2]\n",
    "\n",
    "#     # ---- Stage 1: YOLO ----\n",
    "#     yolo_results = yolo_model.predict(frame_bgr, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "#     yolo_frame = yolo_results[0].plot()\n",
    "#     yolo_path = os.path.join(out_dir, f\"{name}_yolo.png\")\n",
    "#     cv2.imwrite(yolo_path, yolo_frame)\n",
    "\n",
    "#     results_dict = {\n",
    "#         \"boxes\": yolo_results[0].boxes.xyxy.cpu().numpy(),\n",
    "#         \"conf\": yolo_results[0].boxes.conf.cpu().numpy(),\n",
    "#         \"cls\": yolo_results[0].boxes.cls.cpu().numpy()\n",
    "#     }\n",
    "#     with open(os.path.join(out_dir, f\"{name}_yolo_results.pkl\"), \"wb\") as f:\n",
    "#         pickle.dump([results_dict], f)\n",
    "\n",
    "#     # ---- Stage 2: DeOldify ----\n",
    "#     deold = deoldify_inference(frame_rgb)\n",
    "#     deold_bgr = cv2.cvtColor(deold, cv2.COLOR_RGB2BGR)\n",
    "#     deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.png\")\n",
    "#     cv2.imwrite(deoldify_path, deold_bgr)\n",
    "\n",
    "#     # ---- Stage 3: SAM ----\n",
    "#     sam_frames_dir = os.path.join(out_dir, f\"{name}_sam\")\n",
    "#     os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "\n",
    "#     boxes, confs, clses = results_dict[\"boxes\"], results_dict[\"conf\"], results_dict[\"cls\"]\n",
    "#     order = np.argsort(confs)[::-1][:TOP_K_BBOX]\n",
    "#     masks_for_frame = []\n",
    "\n",
    "#     for i in order:\n",
    "#         if int(clses[i]) != 0 or confs[i] < CONF_THRESHOLD:\n",
    "#             continue\n",
    "#         x1, y1, x2, y2 = map(int, boxes[i])\n",
    "#         bw, bh = x2 - x1, y2 - y1\n",
    "#         x1 = max(0, int(x1 - BBOX_ENLARGE * bw))\n",
    "#         y1 = max(0, int(y1 - BBOX_ENLARGE * bh))\n",
    "#         x2 = min(w, int(x2 + BBOX_ENLARGE * bw))\n",
    "#         y2 = min(h, int(y2 + BBOX_ENLARGE * bh))\n",
    "\n",
    "#         crop = frame_bgr[y1:y2, x1:x2]\n",
    "#         if crop.size == 0:\n",
    "#             continue\n",
    "\n",
    "#         crop_path = os.path.join(sam_frames_dir, f\"{name}_box{i}.png\")\n",
    "#         cv2.imwrite(crop_path, crop)\n",
    "\n",
    "#         try:\n",
    "#             seg_path = run_sam_on_frame(crop_path, comfy_server=COMFY)\n",
    "#             seg_img = cv2.imread(seg_path)\n",
    "#             seg_resized = cv2.resize(seg_img, (x2 - x1, y2 - y1))\n",
    "#             mask = np.zeros((h, w), dtype=np.uint8)\n",
    "#             mask[y1:y2, x1:x2] = cv2.cvtColor(seg_resized, cv2.COLOR_BGR2GRAY)\n",
    "#             masks_for_frame.append(mask)\n",
    "#         except Exception as e:\n",
    "#             print(f\"SAM failed on box {i}: {e}\")\n",
    "\n",
    "#     # Save final mask (blank if none)\n",
    "#     if masks_for_frame:\n",
    "#         final_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "#         for m in masks_for_frame:\n",
    "#             final_mask = cv2.bitwise_or(final_mask, m)\n",
    "#     else:\n",
    "#         final_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "#     sam_path = os.path.join(out_dir, f\"{name}_sam.png\")\n",
    "#     cv2.imwrite(sam_path, final_mask)\n",
    "\n",
    "#     # ---- Stage 4: Fusion (exact same logic as run_fusion) ----\n",
    "#     gray = final_mask.copy()\n",
    "#     _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "#     mask_bool = sam_mask > 127\n",
    "\n",
    "#     fusion_frame = frame_bgr.copy()\n",
    "#     fusion_frame[mask_bool] = deold_bgr[mask_bool]\n",
    "\n",
    "#     fusion_path = os.path.join(out_dir, f\"{name}_final.png\")\n",
    "#     cv2.imwrite(fusion_path, fusion_frame)\n",
    "\n",
    "#     print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "#     return {\n",
    "#         \"yolo\": yolo_path,\n",
    "#         \"deoldify\": deoldify_path,\n",
    "#         \"sam\": sam_path,\n",
    "#         \"final\": fusion_path\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "# # ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_image = r\"input_videos/first_frame.jpg\"\n",
    "#     outputs = process_image(input_image)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k, v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2065c-50c7-4359-bf20-b011a278daa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2cc53-f385-4dbd-9bc8-bacc48cc4238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1366641-c52d-428e-8f6b-942a24f45364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd3d16-cdb1-4ea6-ab2d-78b0203d6f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29000e-3ca4-4d94-bebc-679b758a81d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921b31c-4321-4cc2-82d8-e6a911081323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca59a7-f4ef-4876-ab61-5b9ad95c7437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843f168-dce9-4086-b6a6-a7c363246d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2011deaa-ebce-49c6-8b6c-b86e7cb10fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d52a09-c9c7-4dd6-9aaf-56d6a7003f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f7519-8e0a-4007-80e3-ba121e41a155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8818909-473b-49f0-98de-72ee6f8b26a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94faf37-1c77-4b7e-8016-1602c9a44221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    return to_path\n",
    "\n",
    "def run_sam_on_frame(frame_rgb, comfy_server=COMFY):\n",
    "    tmp_path = f\"temp_frame_{uuid.uuid4().hex}.png\"\n",
    "    Image.fromarray(frame_rgb).save(tmp_path)\n",
    "    uploaded = upload_image_to_comfy(tmp_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_loadimage_node(prompt, uploaded)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 60\n",
    "    seg_path = None\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    seg_path = download_image(\n",
    "                        im[\"filename\"], server=comfy_server,\n",
    "                        subfolder=im.get(\"subfolder\", \"\"),\n",
    "                        folder_type=im.get(\"type\", \"output\")\n",
    "                    )\n",
    "                    break\n",
    "        if seg_path: break\n",
    "        time.sleep(0.5)\n",
    "    os.remove(tmp_path)\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    frames_dir = os.path.join(out_dir, f\"{name}_yolo_frames\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    results_per_frame = []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        for frame_idx in range(total_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\")\n",
    "\n",
    "            if os.path.exists(frame_path):\n",
    "                results_per_frame.append(None)  # results loaded separately if needed\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            plotted = results[0].plot()\n",
    "            cv2.imwrite(frame_path, plotted)\n",
    "\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "\n",
    "    # Save video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "    for frame_idx in range(total_frames):\n",
    "        img = cv2.imread(os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\"))\n",
    "        if img is not None:\n",
    "            writer.write(img)\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    frames_dir = os.path.join(out_dir, f\"{name}_deoldify_frames\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        for frame_idx in range(total_frames):\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\")\n",
    "            if os.path.exists(frame_path):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            cv2.imwrite(frame_path, cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "\n",
    "    # Save video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "    for frame_idx in range(total_frames):\n",
    "        img = cv2.imread(os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\"))\n",
    "        if img is not None:\n",
    "            writer.write(img)\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        for frame_idx in range(total_frames):\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\")\n",
    "            if os.path.exists(frame_path):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            cap = cv2.VideoCapture(input_path)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame_bgr = cap.read()\n",
    "            cap.release()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                seg_resized = cv2.resize(seg_img, (width, height)) if seg_img is not None else frame_bgr\n",
    "                cv2.imwrite(frame_path, seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è SAM failed: {e}\")\n",
    "                cv2.imwrite(frame_path, frame_bgr)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "    for frame_idx in range(total_frames):\n",
    "        img = cv2.imread(os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\"))\n",
    "        if img is not None:\n",
    "            writer.write(img)\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)      # original frames\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)   # deoldify video\n",
    "    cap_sam = cv2.VideoCapture(sam_path)          # sam masks\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion: base is ORIGINAL frame\n",
    "            fusion_frame = frame_in.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e9d443-4b24-41da-8769-2f077bffdffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.69s/frame]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outputs written to outputs/Dr - Trim\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/Dr - Trim/Dr - Trim_yolo.mp4\n",
      " - deoldify: outputs/Dr - Trim/Dr - Trim_deoldify.mp4\n",
      " - sam: outputs/Dr - Trim/Dr - Trim_sam.mp4\n",
      " - final: outputs/Dr - Trim/Dr - Trim_final.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # your ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None):\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    if to_path is None:\n",
    "        to_path = filename\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return to_path\n",
    "\n",
    "def run_sam_on_frame(frame_rgb, comfy_server=COMFY):\n",
    "    tmp_path = f\"temp_frame_{uuid.uuid4().hex}.png\"\n",
    "    Image.fromarray(frame_rgb).save(tmp_path)\n",
    "    uploaded = upload_image_to_comfy(tmp_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON,\"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_loadimage_node(prompt, uploaded)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time()+60\n",
    "    seg_path = None\n",
    "    while time.time()<deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    seg_path = download_image(im[\"filename\"], server=comfy_server,\n",
    "                                              subfolder=im.get(\"subfolder\",\"\"),\n",
    "                                              folder_type=im.get(\"type\",\"output\"))\n",
    "                    break\n",
    "        if seg_path: break\n",
    "        time.sleep(0.5)\n",
    "    os.remove(tmp_path)\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Output paths\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer_yolo = cv2.VideoWriter(yolo_path, fourcc, fps, (width,height))\n",
    "    writer_deoldify = cv2.VideoWriter(deoldify_path, fourcc, fps, (width,height))\n",
    "    writer_sam = cv2.VideoWriter(sam_path, fourcc, fps, (width,height))\n",
    "    writer_fusion = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"Processing\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # --- YOLO detections ---\n",
    "            results = yolo_model.predict(frame_bgr, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer_yolo.write(results[0].plot())\n",
    "\n",
    "            # --- DeOldify full-frame ---\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer_deoldify.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # --- SAM segmentation ---\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                if seg_img is None:\n",
    "                    seg_resized = frame_bgr\n",
    "                else:\n",
    "                    seg_resized = cv2.resize(seg_img, (width,height))\n",
    "                writer_sam.write(seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è SAM failed on frame: {e}\")\n",
    "                seg_resized = frame_bgr\n",
    "                writer_sam.write(frame_bgr)\n",
    "\n",
    "            # --- Fusion: apply DeOldify only on SAM ‚à© YOLO ---\n",
    "            fusion_frame = frame_rgb.copy()\n",
    "            if results and results[0].boxes is not None:\n",
    "                boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "                confs = results[0].boxes.conf.cpu().numpy()\n",
    "                classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "                gray = cv2.cvtColor(seg_resized, cv2.COLOR_BGR2GRAY)\n",
    "                _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                for box, conf, cls in zip(boxes, confs, classes):\n",
    "                    if int(cls) != 0 or conf < CONF_THRESHOLD:\n",
    "                        continue  # only person class with conf >= 0.6\n",
    "\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "                    yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "                    intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "                    mask_bool = intersect > 127\n",
    "\n",
    "                    fusion_frame[mask_bool] = deold[mask_bool]\n",
    "\n",
    "            writer_fusion.write(cv2.cvtColor(fusion_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer_yolo.release()\n",
    "    writer_deoldify.release()\n",
    "    writer_sam.release()\n",
    "    writer_fusion.release()\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Example call ---\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/Dr - Trim.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k,v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d64533-9e6c-42df-bceb-dac90fe57a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "\n",
    "\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6c1135-7dd9-4a60-845c-2cb1eef08f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/deoldify/fastai/data_block.py:451: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\n",
      "  warn(\"Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\")\n",
      "/opt/deoldify/fastai/data_block.py:453: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  warn(\"\"\"Your validation set is empty. If this is by design, use `split_none()`\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/opt/deoldify/fastai/basic_train.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(tmp_file)\n",
      "/opt/deoldify/fastai/basic_train.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(source, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None):\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    if to_path is None:\n",
    "        to_path = filename\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return to_path\n",
    "\n",
    "def run_sam_on_frame(frame_rgb, comfy_server=COMFY):\n",
    "    tmp_path = f\"temp_frame_{uuid.uuid4().hex}.png\"\n",
    "    Image.fromarray(frame_rgb).save(tmp_path)\n",
    "    uploaded = upload_image_to_comfy(tmp_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON,\"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_loadimage_node(prompt, uploaded)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time()+60\n",
    "    seg_path = None\n",
    "    while time.time()<deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    seg_path = download_image(im[\"filename\"], server=comfy_server,\n",
    "                                              subfolder=im.get(\"subfolder\",\"\"),\n",
    "                                              folder_type=im.get(\"type\",\"output\"))\n",
    "                    break\n",
    "        if seg_path: break\n",
    "        time.sleep(0.5)\n",
    "    os.remove(tmp_path)\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width,height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            results_per_frame.append(results[0])\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                seg_resized = cv2.resize(seg_img, (width,height)) if seg_img is not None else frame_bgr\n",
    "                writer.write(seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è SAM failed: {e}\")\n",
    "                writer.write(frame_bgr)\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # reload DeOldify\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "\n",
    "            # reload SAM\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                gray = cv2.cvtColor(seg_img, cv2.COLOR_BGR2GRAY)\n",
    "                _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "            except:\n",
    "                sam_mask = np.zeros((height,width), dtype=np.uint8)\n",
    "\n",
    "            # YOLO boxes\n",
    "            fusion_frame = frame_rgb.copy()\n",
    "            if frame_idx < len(yolo_results):\n",
    "                boxes = yolo_results[frame_idx].boxes.xyxy.cpu().numpy()\n",
    "                confs = yolo_results[frame_idx].boxes.conf.cpu().numpy()\n",
    "                classes = yolo_results[frame_idx].boxes.cls.cpu().numpy()\n",
    "                for box, conf, cls in zip(boxes, confs, classes):\n",
    "                    if int(cls) != 0 or conf < CONF_THRESHOLD:\n",
    "                        continue\n",
    "                    x1,y1,x2,y2 = map(int, box)\n",
    "                    yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "                    yolo_mask[y1:y2,x1:x2] = 255\n",
    "                    intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "                    mask_bool = intersect > 127\n",
    "                    fusion_frame[mask_bool] = deold[mask_bool]\n",
    "\n",
    "            writer.write(cv2.cvtColor(fusion_frame, cv2.COLOR_RGB2BGR))\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# # ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/THATHA MANAVADU colored Trim.mp4\"\n",
    "#     outputs = process_video(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k,v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928f6e6c-0b0f-4dbf-887b-3cb7d3ba04c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4baa01ab-4765-41b1-bab6-7996bf83523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n",
      "[CACHE] Using cached YOLO: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_yolo.mp4\n",
      "[CACHE] Using cached DeOldify: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_deoldify.mp4\n",
      "[CACHE] Using cached SAM: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_sam.mp4\n",
      "[CACHE] Using cached Fusion: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_final.mp4\n",
      "[INFO] Outputs written to outputs/THATHA MANAVADU colored Trim\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_yolo.mp4\n",
      " - deoldify: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_deoldify.mp4\n",
      " - sam: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_sam.mp4\n",
      " - final: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_final.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    if os.path.exists(yolo_path):\n",
    "        print(f\"[CACHE] Using cached YOLO: {yolo_path}\")\n",
    "        return yolo_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return yolo_path\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                seg_resized = cv2.resize(seg_img, (width, height)) if seg_img is not None else frame_bgr\n",
    "                writer.write(seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è SAM failed: {e}\")\n",
    "                writer.write(frame_bgr)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(yolo_path, deoldify_path, sam_path, out_dir, name):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_yolo = cv2.VideoCapture(yolo_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_yolo.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_yolo.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_yolo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        cap_yolo.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        for _ in range(total_frames):\n",
    "            ret1, frame_yolo = cap_yolo.read()\n",
    "            ret2, frame_deold = cap_deold.read()\n",
    "            ret3, frame_sam = cap_sam.read()\n",
    "            if not (ret1 and ret2 and ret3):\n",
    "                break\n",
    "\n",
    "            # Build mask from SAM video\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "            mask_bool = sam_mask > 127\n",
    "\n",
    "            # Fusion: base is YOLO frame, apply DeOldify where mask is valid\n",
    "            fusion_frame = frame_yolo.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_yolo.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_fusion(input_path, yolo_results, deoldify_path, sam_path, out_dir, name, conf_thresh=0.6):\n",
    "    \"\"\"\n",
    "    Fusion: Apply DeOldify color only where SAM mask ‚à© YOLO(person, conf>=0.6) overlap.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): original input video path (for reference size/fps)\n",
    "        yolo_results (list): list of YOLO results per frame (from run_yolo)\n",
    "        deoldify_path (str): path to DeOldify output video\n",
    "        sam_path (str): path to SAM output video\n",
    "        out_dir (str): output directory\n",
    "        name (str): base name\n",
    "        conf_thresh (float): YOLO confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        str: path to fusion video\n",
    "    \"\"\"\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    # Open video sources\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_deold.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_deold.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_deold.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # --- Build SAM mask ---\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # --- Build YOLO mask ---\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            boxes = yolo_results[frame_idx].boxes.xyxy.cpu().numpy()\n",
    "            confs = yolo_results[frame_idx].boxes.conf.cpu().numpy()\n",
    "            classes = yolo_results[frame_idx].boxes.cls.cpu().numpy()\n",
    "\n",
    "            for box, conf, cls in zip(boxes, confs, classes):\n",
    "                if int(cls) != 0 or conf < conf_thresh:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # --- Intersection mask (SAM ‚à© YOLO) ---\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # --- Apply DeOldify only where intersection is True ---\n",
    "            fusion_frame = frame_deold.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, yolo_path, deoldify_path, sam_path, out_dir, name)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/THATHA MANAVADU colored Trim.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26b8a4f-b1e5-48f1-9ba9-e521f1113107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Using: NVIDIA GeForce RTX 4060 Ti\n",
      "[INFO] Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1871/1871 [00:47<00:00, 39.15frame/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CACHE] Using cached DeOldify: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_deoldify.mp4\n",
      "[CACHE] Using cached SAM: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_sam.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusion: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1871/1871 [00:24<00:00, 75.69frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fusion video saved: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_final.mp4\n",
      "[INFO] Outputs written to outputs/THATHA MANAVADU colored Trim\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_yolo.mp4\n",
      " - deoldify: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_deoldify.mp4\n",
      " - sam: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_sam.mp4\n",
      " - final: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_final.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "\n",
    "# yolo_model = YOLO(YOLO_MODEL_PATH).to(\"cuda\")\n",
    "# print(f\"YOLO running on: {yolo_model.device}\")\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            # Store only necessary info to reduce memory\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Here, you should integrate your SAM inference instead of dummy mask\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            seg_resized = frame_bgr  # fallback\n",
    "            writer.write(seg_resized)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(yolo_results, deoldify_path, sam_path, out_dir, name):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_deold.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_deold.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_deold.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion: apply DeOldify where intersection\n",
    "            fusion_frame = frame_deold.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(yolo_results, deoldify_path, sam_path, out_dir, name)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/THATHA MANAVADU colored Trim.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73751eb-8ddc-4d0d-be91-a2044e559815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Using: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9734336a-c83d-441d-88e4-fbc0ed4bc41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/deoldify/fastai/data_block.py:451: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\n",
      "  warn(\"Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\")\n",
      "/opt/deoldify/fastai/data_block.py:453: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  warn(\"\"\"Your validation set is empty. If this is by design, use `split_none()`\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/opt/deoldify/fastai/basic_train.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(tmp_file)\n",
      "/opt/deoldify/fastai/basic_train.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(source, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None):\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    if to_path is None:\n",
    "        to_path = filename\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return to_path\n",
    "\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    # If not specified, save under OUTPUT_ROOT/comfy_downloads/\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    return to_path\n",
    "\n",
    "\n",
    "def run_sam_on_frame(frame_rgb, comfy_server=COMFY):\n",
    "    tmp_path = f\"temp_frame_{uuid.uuid4().hex}.png\"\n",
    "    Image.fromarray(frame_rgb).save(tmp_path)\n",
    "    uploaded = upload_image_to_comfy(tmp_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_loadimage_node(prompt, uploaded)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 60\n",
    "    seg_path = None\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    seg_path = download_image(\n",
    "                        im[\"filename\"], server=comfy_server,\n",
    "                        subfolder=im.get(\"subfolder\", \"\"),\n",
    "                        folder_type=im.get(\"type\", \"output\")\n",
    "                    )\n",
    "                    break\n",
    "        if seg_path: break\n",
    "        time.sleep(0.5)\n",
    "    os.remove(tmp_path)\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                seg_resized = cv2.resize(seg_img, (width, height)) if seg_img is not None else frame_bgr\n",
    "                writer.write(seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è SAM failed: {e}\")\n",
    "                writer.write(frame_bgr)\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)       # original\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)    # colorized\n",
    "    cap_sam = cv2.VideoCapture(sam_path)           # masks\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        len(yolo_results)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_bgr = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # original gray\n",
    "            frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "            frame_original = cv2.cvtColor(frame_gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion\n",
    "            fusion_frame = frame_original.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)      # original frames\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)   # deoldify video\n",
    "    cap_sam = cv2.VideoCapture(sam_path)          # sam masks\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion: base is ORIGINAL frame\n",
    "            fusion_frame = frame_in.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "#     outputs = process_video(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k, v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c52772-179c-482d-9ac4-67d132d0bada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
