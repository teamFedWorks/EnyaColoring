{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0224de9-0790-4049-a8c4-909217215ae9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "video without yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "252aa25d-175a-4357-abe6-f2c8fab0feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Using: NVIDIA GeForce RTX 4060 Ti\n",
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesArmsFaceNeck.json\"\n",
    "GDINO_PROMPT = \"clothes\" # grounding dino prompt\n",
    "GDINO_THRESHOLD = 0.35   # grounding dino threshold\n",
    "# =================\n",
    "\n",
    "\n",
    "def patch_groundingdino_node(prompt_dict, new_prompt=None, new_threshold=None):\n",
    "    \"\"\"Patch GroundingDinoSAMSegment node with new prompt/threshold values.\"\"\"\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\", \"\").lower().startswith(\"groundingdinosamsegment\"):\n",
    "            if new_prompt is not None:\n",
    "                node[\"inputs\"][\"prompt\"] = new_prompt\n",
    "            if new_threshold is not None:\n",
    "                node[\"inputs\"][\"threshold\"] = new_threshold\n",
    "            return True\n",
    "    return False\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    return to_path\n",
    "\n",
    "\n",
    "def run_sam_on_frame(frame_path, comfy_server=COMFY):\n",
    "    \"\"\"Send one frame through ComfyUI workflow and return saved ComfyUI_* path.\"\"\"\n",
    "    uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "    # Load workflow JSON\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # Patch LoadImage node\n",
    "    if not patch_loadimage_node(prompt, uploaded):\n",
    "        raise RuntimeError(\"Could not patch LoadImage node in workflow JSON.\")\n",
    "\n",
    "    patch_groundingdino_node(prompt, new_prompt=GDINO_PROMPT, new_threshold=GDINO_THRESHOLD)\n",
    "    # Queue\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 600  # up to 10 min per frame\n",
    "    seg_path = None\n",
    "\n",
    "    # Poll history until output is ready\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "                    # Save as ComfyUI_<originalname>.png in same folder\n",
    "                    base = os.path.splitext(os.path.basename(frame_path))[0]\n",
    "                    save_dir = os.path.dirname(frame_path)\n",
    "                    out_path = os.path.join(save_dir, f\"ComfyUI_{base}.png\")\n",
    "                    seg_path = download_image(fn, server=comfy_server,\n",
    "                                              subfolder=sub, folder_type=typ,\n",
    "                                              to_path=out_path)\n",
    "                    break\n",
    "        if seg_path:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if not seg_path:\n",
    "        raise RuntimeError(f\"No outputs from ComfyUI for {frame_path}\")\n",
    "\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "def run_sam_on_frame_multi(frame_path, comfy_server=COMFY):\n",
    "    \"\"\"\n",
    "    Send one frame through ComfyUI workflow and return 4 mask paths:\n",
    "    clothes, arms, necks, faces.\n",
    "    Then compute two final masks:\n",
    "      1. clothes - (arms+faces+necks)\n",
    "      2. (arms+necks)\n",
    "    \"\"\"\n",
    "    uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "    # Load workflow JSON\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # Patch LoadImage node\n",
    "    if not patch_loadimage_node(prompt, uploaded):\n",
    "        raise RuntimeError(\"Could not patch LoadImage node in workflow JSON.\")\n",
    "\n",
    "    # Queue to Comfy\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 600\n",
    "    outputs = {}\n",
    "\n",
    "    # Poll until we get outputs\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_id, node_out in item[\"outputs\"].items():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "\n",
    "                    save_dir = os.path.dirname(frame_path)\n",
    "                    label = None\n",
    "                    if node_id == \"12\": label = \"clothes\"\n",
    "                    elif node_id == \"13\": label = \"arms\"\n",
    "                    elif node_id == \"14\": label = \"necks\"\n",
    "                    elif node_id == \"15\": label = \"faces\"\n",
    "                    else: continue\n",
    "\n",
    "                    out_path = os.path.join(save_dir, f\"{label}_{os.path.basename(frame_path)}\")\n",
    "                    outputs[label] = download_image(fn, server=comfy_server,\n",
    "                                                    subfolder=sub, folder_type=typ,\n",
    "                                                    to_path=out_path)\n",
    "        if len(outputs) == 4:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if len(outputs) < 4:\n",
    "        raise RuntimeError(f\"Missing outputs from ComfyUI for {frame_path}: {outputs.keys()}\")\n",
    "\n",
    "    # --- Build final composite masks ---\n",
    "    clothes = cv2.imread(outputs[\"clothes\"], cv2.IMREAD_GRAYSCALE)\n",
    "    arms   = cv2.imread(outputs[\"arms\"], cv2.IMREAD_GRAYSCALE)\n",
    "    necks  = cv2.imread(outputs[\"necks\"], cv2.IMREAD_GRAYSCALE)\n",
    "    faces  = cv2.imread(outputs[\"faces\"], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    union_body = cv2.bitwise_or(arms, necks)\n",
    "    union_body_face = cv2.bitwise_or(union_body, faces)\n",
    "\n",
    "    clothes_final = cv2.subtract(clothes, union_body_face)    # Clothes minus (arms+faces+necks)\n",
    "    body_final = union_body                                   # Arms+necks\n",
    "\n",
    "    # Save them\n",
    "    clothes_path = os.path.join(save_dir, f\"ClothesFinal_{os.path.basename(frame_path)}\")\n",
    "    body_path    = os.path.join(save_dir, f\"BodyFinal_{os.path.basename(frame_path)}\")\n",
    "    cv2.imwrite(clothes_path, clothes_final)\n",
    "    cv2.imwrite(body_path, body_final)\n",
    "\n",
    "    outputs[\"clothes_final\"] = clothes_path\n",
    "    outputs[\"body_final\"] = body_path\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM (extract → ComfyUI per frame → video) ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM video: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    # --- Step 1: Extract all frames first ---\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Extracting frames\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_file = os.path.join(sam_frames_dir, f\"frame_{idx:06d}.png\")\n",
    "            if not os.path.exists(frame_file):\n",
    "                cv2.imwrite(frame_file, frame_bgr)\n",
    "            idx += 1\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "\n",
    "    # --- Step 2: Run ComfyUI on each frame ---\n",
    "    frame_files = sorted([f for f in os.listdir(sam_frames_dir) if f.startswith(\"frame_\")])\n",
    "    for fname in tqdm(frame_files, desc=\"Processing with ComfyUI\", unit=\"frame\"):\n",
    "        frame_file = os.path.join(sam_frames_dir, fname)\n",
    "        out_file = os.path.join(sam_frames_dir, f\"ComfyUI_{fname}\")\n",
    "        if os.path.exists(out_file):\n",
    "            continue\n",
    "        try:\n",
    "            run_sam_on_frame(frame_file, comfy_server=COMFY)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ SAM failed on {fname}: {e}\")\n",
    "\n",
    "    # --- Step 3: Collect only ComfyUI_* frames ---\n",
    "    sam_files = sorted([f for f in os.listdir(sam_frames_dir) if f.startswith(\"ComfyUI_\")])\n",
    "\n",
    "    # --- Step 4: Combine into video ---\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for seg_file in tqdm(sam_files, desc=\"Building SAM video\", unit=\"frame\"):\n",
    "        img = cv2.imread(os.path.join(sam_frames_dir, seg_file))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img_resized = cv2.resize(img, (width, height))\n",
    "        writer.write(img_resized)\n",
    "\n",
    "    writer.release()\n",
    "    print(f\"[INFO] SAM video saved: {sam_path}\")\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "\n",
    "def run_sam_multi(input_path, out_dir, name):\n",
    "    \"\"\"\n",
    "    Runs ComfyUI workflow on all frames and produces 2 final mask videos:\n",
    "      - ClothesFinal video\n",
    "      - BodyFinal video\n",
    "    \"\"\"\n",
    "    frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    clothes_path = os.path.join(out_dir, f\"{name}_clothesFinal.mp4\")\n",
    "    body_path    = os.path.join(out_dir, f\"{name}_bodyFinal.mp4\")\n",
    "\n",
    "    if os.path.exists(clothes_path) and os.path.exists(body_path):\n",
    "        print(f\"[CACHE] Using cached SAM videos: {clothes_path}, {body_path}\")\n",
    "        return clothes_path, body_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Extract frames\n",
    "    frame_files = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        fpath = os.path.join(frames_dir, f\"frame_{idx:06d}.png\")\n",
    "        cv2.imwrite(fpath, frame)\n",
    "        frame_files.append(fpath)\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "\n",
    "    clothes_writer = cv2.VideoWriter(clothes_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "    body_writer    = cv2.VideoWriter(body_path,    cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    for f in tqdm(frame_files, desc=\"Comfy Multi-Mask\", unit=\"frame\"):\n",
    "        try:\n",
    "            outs = run_sam_on_frame_multi(f, comfy_server=COMFY)\n",
    "\n",
    "            clothes = cv2.imread(outs[\"clothes_final\"], cv2.IMREAD_GRAYSCALE)\n",
    "            body    = cv2.imread(outs[\"body_final\"], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Resize to match original video dimensions\n",
    "            clothes = cv2.resize(clothes, (width, height))\n",
    "            body    = cv2.resize(body, (width, height))\n",
    "\n",
    "            # Binarize masks\n",
    "            _, clothes = cv2.threshold(clothes, 127, 255, cv2.THRESH_BINARY)\n",
    "            _, body    = cv2.threshold(body, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Convert to 3-channel (so OpenCV writes consistent frames)\n",
    "            clothes_color = cv2.cvtColor(clothes, cv2.COLOR_GRAY2BGR)\n",
    "            body_color    = cv2.cvtColor(body, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            clothes_writer.write(clothes_color)\n",
    "            body_writer.write(body_color)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed on {f}: {e}\")\n",
    "\n",
    "    clothes_writer.release()\n",
    "    body_writer.release()\n",
    "\n",
    "    print(f\"[INFO] SAM multi videos saved: {clothes_path}, {body_path}\")\n",
    "    return clothes_path, body_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM (with frame folder + resume support) ----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)      # original frames\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)   # deoldify video\n",
    "    cap_sam = cv2.VideoCapture(sam_path)          # sam masks\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion: base is ORIGINAL frame\n",
    "            fusion_frame = frame_in.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "def run_fusion_single(input_path, out_dir, name, yolo_results, deoldify_path, mask_path, label):\n",
    "    \"\"\"\n",
    "    Fuse one mask (either ClothesFinal or BodyFinal) with deoldify result.\n",
    "    \"\"\"\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_{label}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_mask  = cv2.VideoCapture(mask_path)\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = cv2.VideoWriter(fusion_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    total_frames = min(int(cap_input.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       int(cap_deold.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       int(cap_mask.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "    for _ in tqdm(range(total_frames), desc=f\"Fusion-{label}\", unit=\"frame\"):\n",
    "        ret_in, frame_in = cap_input.read()\n",
    "        ret_de, frame_de = cap_deold.read()\n",
    "        ret_m, mask_img  = cap_mask.read()\n",
    "        if not (ret_in and ret_de and ret_m): break\n",
    "\n",
    "        # Convert mask to binary\n",
    "        mask = cv2.cvtColor(mask_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, mask_bin = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        mask_bool = mask_bin > 127\n",
    "\n",
    "        # Apply fusion\n",
    "        fusion_frame = frame_in.copy()\n",
    "        fusion_frame[mask_bool] = frame_de[mask_bool]\n",
    "\n",
    "        writer.write(fusion_frame)\n",
    "\n",
    "    cap_input.release(); cap_deold.release(); cap_mask.release(); writer.release()\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "def run_fusion_single(input_path, out_dir, name, yolo_results, deoldify_path, mask_path, label):\n",
    "    \"\"\"\n",
    "    Fuse one mask (either ClothesFinal or BodyFinal) with deoldify result,\n",
    "    applying the SAME logic as run_fusion: intersection of YOLO and SAM masks.\n",
    "    \"\"\"\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_{label}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_mask  = cv2.VideoCapture(mask_path)\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = cv2.VideoWriter(fusion_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    total_frames = min(int(cap_input.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       int(cap_deold.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       int(cap_mask.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       len(yolo_results))\n",
    "\n",
    "    frame_idx = 0\n",
    "    for _ in tqdm(range(total_frames), desc=f\"Fusion-{label}\", unit=\"frame\"):\n",
    "        ret_in, frame_in = cap_input.read()\n",
    "        ret_de, frame_de = cap_deold.read()\n",
    "        ret_m, mask_img  = cap_mask.read()\n",
    "        if not (ret_in and ret_de and ret_m): break\n",
    "\n",
    "        # --- SAM mask (from clothes/body video) ---\n",
    "        sam_gray = cv2.cvtColor(mask_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, sam_mask = cv2.threshold(sam_gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # --- YOLO mask (same logic as run_fusion) ---\n",
    "        yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "        for box, conf, cls in zip(\n",
    "            yolo_results[frame_idx][\"boxes\"],\n",
    "            yolo_results[frame_idx][\"conf\"],\n",
    "            yolo_results[frame_idx][\"cls\"]\n",
    "        ):\n",
    "            if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                continue\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(width, x2), min(height, y2)\n",
    "            yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "        # --- Intersection YOLO ∩ SAM ---\n",
    "        intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "        mask_bool = intersect > 127\n",
    "\n",
    "        # --- Apply fusion (overlay DeOldify where mask == 1) ---\n",
    "        fusion_frame = frame_in.copy()\n",
    "        fusion_frame[mask_bool] = frame_de[mask_bool]\n",
    "\n",
    "        writer.write(fusion_frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap_input.release(); cap_deold.release(); cap_mask.release(); writer.release()\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "def process_video_multi(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Stage 1: YOLO\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "\n",
    "    # Stage 2: DeOldify\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "\n",
    "    # Stage 3: SAM multi-mask\n",
    "    clothes_mask_path, body_mask_path = run_sam_multi(input_path, out_dir, name)\n",
    "\n",
    "    # Stage 4: Fusion (separate)\n",
    "    clothes_final = run_fusion_single(input_path, out_dir, name, yolo_results, deoldify_path, clothes_mask_path, \"clothes\")\n",
    "    body_final    = run_fusion_single(input_path, out_dir, name, yolo_results, deoldify_path, body_mask_path, \"body\")\n",
    "\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"clothes_mask\": clothes_mask_path,\n",
    "        \"body_mask\": body_mask_path,\n",
    "        \"clothes_final\": clothes_final,\n",
    "        \"body_final\": body_final\n",
    "    }\n",
    "\n",
    "\n",
    "def process_video_cached(input_path):\n",
    "    \"\"\"\n",
    "    Cached wrapper around process_video().\n",
    "    Returns only the final fusion video path.\n",
    "    \"\"\"\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    final_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"[CACHE] Final output exists: {final_path}\")\n",
    "        return final_path\n",
    "\n",
    "    outputs = process_video(input_path)\n",
    "    return outputs[\"final\"]\n",
    "\n",
    "\n",
    "# # # ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "#     #outputs = process_video(input_video)\n",
    "#     outputs = process_video_multi(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k, v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98076cea-4e22-4935-9305-b0d771c47ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/deoldify/fastai/data_block.py:451: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\n",
      "  warn(\"Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\")\n",
      "/opt/deoldify/fastai/data_block.py:453: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  warn(\"\"\"Your validation set is empty. If this is by design, use `split_none()`\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/opt/deoldify/fastai/basic_train.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(tmp_file)\n",
      "/opt/deoldify/fastai/basic_train.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(source, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, torch, json, uuid, time, pickle, requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"\n",
    "WORKFLOW_JSON = \"ClothesArmsFaceNeck.json\"\n",
    "# =================\n",
    "\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- Helpers for DeOldify ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI Helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return to_path\n",
    "\n",
    "\n",
    "# ---- Run ComfyUI for multi-prompt masks ----\n",
    "def run_multi_masks(frame_path, comfy_server=COMFY):\n",
    "    uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # Patch input image for all LoadImage references\n",
    "    for node in prompt.values():\n",
    "        if node.get(\"class_type\", \"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = uploaded\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 600\n",
    "    masks = {}\n",
    "\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_id, node_out in item[\"outputs\"].items():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "                    out_path = os.path.join(os.path.dirname(frame_path), f\"{node_id}_{os.path.basename(frame_path)}\")\n",
    "                    masks[node_id] = download_image(fn, server=comfy_server, subfolder=sub, folder_type=typ, to_path=out_path)\n",
    "        if masks:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return masks   # dict: {node_id: path}\n",
    "\n",
    "\n",
    "def run_multi_masks(frame_path, comfy_server=COMFY):\n",
    "    \"\"\"\n",
    "    Runs the ComfyUI workflow and returns masks with semantic names\n",
    "    instead of numeric node IDs.\n",
    "    Returns dict: {\"clothes\": path, \"arms\": path, \"neck\": path, \"faces\": path}\n",
    "    \"\"\"\n",
    "    uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # Patch input image for all LoadImage nodes\n",
    "    for node in prompt.values():\n",
    "        if node.get(\"class_type\", \"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = uploaded\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 600\n",
    "    masks = {}\n",
    "\n",
    "    # Mapping from node_id to semantic name (adjust if your JSON changes)\n",
    "    id_to_name = {\n",
    "        \"2\": \"clothes\",\n",
    "        \"6\": \"arms\",\n",
    "        \"8\": \"neck\",\n",
    "        \"10\": \"faces\"\n",
    "    }\n",
    "\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_id, node_out in item[\"outputs\"].items():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "                    out_path = os.path.join(os.path.dirname(frame_path),\n",
    "                                            f\"{id_to_name.get(node_id, node_id)}_{os.path.basename(frame_path)}\")\n",
    "                    dl_path = download_image(fn, server=comfy_server, subfolder=sub,\n",
    "                                             folder_type=typ, to_path=out_path)\n",
    "\n",
    "                    if node_id in id_to_name:\n",
    "                        masks[id_to_name[node_id]] = dl_path\n",
    "                    else:\n",
    "                        masks[node_id] = dl_path  # fallback if unmapped\n",
    "        if masks:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return masks   # {\"clothes\": path, \"arms\": path, \"neck\": path, \"faces\": path}\n",
    "\n",
    "\n",
    "# ---- Stage SAM + Mask Fusion ----\n",
    "def run_sam_and_fuse(input_path, out_dir, name):\n",
    "    sam_frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    idx = 0\n",
    "    finalA_path = os.path.join(out_dir, f\"{name}_maskA.mp4\")  # clothes+faces+arms+neck\n",
    "    finalB_path = os.path.join(out_dir, f\"{name}_maskB.mp4\")  # arms+neck\n",
    "\n",
    "    writerA = cv2.VideoWriter(finalA_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "    writerB = cv2.VideoWriter(finalB_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"SAM+Fusion\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_file = os.path.join(sam_frames_dir, f\"frame_{idx:06d}.png\")\n",
    "            cv2.imwrite(frame_file, frame_bgr)\n",
    "\n",
    "            try:\n",
    "                masks = run_multi_masks(frame_file, comfy_server=COMFY)\n",
    "                # Load each mask\n",
    "                mask_imgs = {}\n",
    "                for k, v in masks.items():\n",
    "                    img = cv2.imread(v, cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is not None:\n",
    "                        mask_imgs[k] = cv2.resize(img, (width, height))\n",
    "\n",
    "                # Fuse masks\n",
    "                mask_clothes = mask_imgs.get(\"2\", np.zeros((height, width), np.uint8))\n",
    "                mask_arms    = mask_imgs.get(\"6\", np.zeros((height, width), np.uint8))\n",
    "                mask_neck    = mask_imgs.get(\"8\", np.zeros((height, width), np.uint8))\n",
    "                mask_faces   = mask_imgs.get(\"10\", np.zeros((height, width), np.uint8))\n",
    "\n",
    "                finalA = cv2.bitwise_or(mask_clothes,\n",
    "                         cv2.bitwise_or(mask_arms,\n",
    "                         cv2.bitwise_or(mask_neck, mask_faces)))\n",
    "                finalB = cv2.bitwise_or(mask_arms, mask_neck)\n",
    "\n",
    "                writerA.write(cv2.cvtColor(finalA, cv2.COLOR_GRAY2BGR))\n",
    "                writerB.write(cv2.cvtColor(finalB, cv2.COLOR_GRAY2BGR))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Mask fusion failed on frame {idx}: {e}\")\n",
    "\n",
    "            idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writerA.release()\n",
    "    writerB.release()\n",
    "\n",
    "    return finalA_path, finalB_path\n",
    "\n",
    "\n",
    "# ---- Fusion with DeOldify ----\n",
    "def run_fusion_with_masks(input_path, deoldify_path, mask_path, out_path):\n",
    "    cap_in = cv2.VideoCapture(input_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_mask = cv2.VideoCapture(mask_path)\n",
    "\n",
    "    fps = int(cap_in.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_in.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_in.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    while True:\n",
    "        ret_in, f_in = cap_in.read()\n",
    "        ret_de, f_de = cap_deold.read()\n",
    "        ret_m, f_m = cap_mask.read()\n",
    "        if not (ret_in and ret_de and ret_m):\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(f_m, cv2.COLOR_BGR2GRAY)\n",
    "        _, mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "        mask_bool = mask > 127\n",
    "\n",
    "        fused = f_in.copy()\n",
    "        fused[mask_bool] = f_de[mask_bool]\n",
    "        writer.write(fused)\n",
    "\n",
    "    cap_in.release()\n",
    "    cap_deold.release()\n",
    "    cap_mask.release()\n",
    "    writer.release()\n",
    "\n",
    "\n",
    "# ---- Main ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: DeOldify entire video\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if not os.path.exists(deoldify_path):\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        writer = cv2.VideoWriter(deoldify_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "        cap.release(); writer.release()\n",
    "\n",
    "    # Step 2: Run SAM + fuse masks\n",
    "    maskA_path, maskB_path = run_sam_and_fuse(input_path, out_dir, name)\n",
    "\n",
    "    # Step 3: Apply fusion twice\n",
    "    finalA = os.path.join(out_dir, f\"{name}_finalA.mp4\")\n",
    "    finalB = os.path.join(out_dir, f\"{name}_finalB.mp4\")\n",
    "\n",
    "    run_fusion_with_masks(input_path, deoldify_path, maskA_path, finalA)\n",
    "    run_fusion_with_masks(input_path, deoldify_path, maskB_path, finalB)\n",
    "\n",
    "    print(f\"[INFO] Final outputs saved:\\n - {finalA}\\n - {finalB}\")\n",
    "    return finalA, finalB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # # ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "#     #outputs = process_video(input_video)\n",
    "#     outputs = process_video_multi(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k, v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65964705-4a2d-4c71-ac29-2a23563a1fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8115494-e9a9-4019-81a8-60949c72d95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1487b369-fa62-48ad-9846-ff7349aa9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Using:\", torch.cuda.get_device_name(0))\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# from tqdm import tqdm\n",
    "# from ultralytics import YOLO\n",
    "# from deoldify.visualize import get_image_colorizer\n",
    "# from deoldify import device\n",
    "# from deoldify.device_id import DeviceId\n",
    "# from PIL import Image\n",
    "# import uuid, json, requests, time\n",
    "\n",
    "# # ==== CONFIG ====\n",
    "# YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "# CONF_THRESHOLD = 0.6\n",
    "# OUTPUT_ROOT = \"outputs\"\n",
    "# COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "# WORKFLOW_JSON = \"ClothesArmsFaceNeck.json\"\n",
    "# GDINO_PROMPT = \"clothes\" # grounding dino prompt\n",
    "# GDINO_THRESHOLD = 0.35   # grounding dino threshold\n",
    "# # =================\n",
    "\n",
    "\n",
    "# def patch_groundingdino_node(prompt_dict, new_prompt=None, new_threshold=None):\n",
    "#     \"\"\"Patch GroundingDinoSAMSegment node with new prompt/threshold values.\"\"\"\n",
    "#     for node in prompt_dict.values():\n",
    "#         if node.get(\"class_type\", \"\").lower().startswith(\"groundingdinosamsegment\"):\n",
    "#             if new_prompt is not None:\n",
    "#                 node[\"inputs\"][\"prompt\"] = new_prompt\n",
    "#             if new_threshold is not None:\n",
    "#                 node[\"inputs\"][\"threshold\"] = new_threshold\n",
    "#             return True\n",
    "#     return False\n",
    "# # =================\n",
    "\n",
    "# # ---- Setup DeOldify ----\n",
    "# device.set(device=DeviceId.GPU0)\n",
    "# colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# # ---- Setup YOLO ----\n",
    "# device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"[INFO] Using device: {device_str}\")\n",
    "# yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# # ---- DeOldify inference ----\n",
    "# def deoldify_inference(frame_rgb):\n",
    "#     pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "#     ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "#     return np.array(ret)\n",
    "\n",
    "\n",
    "# # ---- ComfyUI helpers ----\n",
    "# def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "#     if dest_name is None:\n",
    "#         dest_name = os.path.basename(local_path)\n",
    "#     with open(local_path, \"rb\") as f:\n",
    "#         files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "#         data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "#         r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "#         r.raise_for_status()\n",
    "#     return dest_name\n",
    "\n",
    "# def patch_loadimage_node(prompt_dict, new_filename):\n",
    "#     for node in prompt_dict.values():\n",
    "#         if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "#             node[\"inputs\"][\"image\"] = new_filename\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def queue_prompt(prompt_dict, server=COMFY):\n",
    "#     client_id = str(uuid.uuid4())\n",
    "#     r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "#     r.raise_for_status()\n",
    "#     return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "# def get_history(prompt_id, server=COMFY):\n",
    "#     r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "#     r.raise_for_status()\n",
    "#     return r.json()\n",
    "\n",
    "# def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "#     if save_dir is None:\n",
    "#         save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "#     r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "#     r.raise_for_status()\n",
    "\n",
    "#     if to_path is None:\n",
    "#         to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "#     with open(to_path, \"wb\") as f:\n",
    "#         f.write(r.content)\n",
    "\n",
    "#     return to_path\n",
    "\n",
    "\n",
    "# def run_sam_on_frame(frame_path, comfy_server=COMFY):\n",
    "#     \"\"\"Send one frame through ComfyUI workflow and return saved ComfyUI_* path.\"\"\"\n",
    "#     uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "#     # Load workflow JSON\n",
    "#     with open(WORKFLOW_JSON, \"r\") as f:\n",
    "#         prompt = json.load(f)\n",
    "\n",
    "#     # Patch LoadImage node\n",
    "#     if not patch_loadimage_node(prompt, uploaded):\n",
    "#         raise RuntimeError(\"Could not patch LoadImage node in workflow JSON.\")\n",
    "\n",
    "#     patch_groundingdino_node(prompt, new_prompt=GDINO_PROMPT, new_threshold=GDINO_THRESHOLD)\n",
    "#     # Queue\n",
    "#     prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "#     deadline = time.time() + 600  # up to 10 min per frame\n",
    "#     seg_path = None\n",
    "\n",
    "#     # Poll history until output is ready\n",
    "#     while time.time() < deadline:\n",
    "#         hist = get_history(prompt_id, server=comfy_server)\n",
    "#         item = hist.get(prompt_id)\n",
    "#         if item and \"outputs\" in item:\n",
    "#             for node_out in item[\"outputs\"].values():\n",
    "#                 for im in node_out.get(\"images\", []):\n",
    "#                     fn = im[\"filename\"]\n",
    "#                     sub = im.get(\"subfolder\", \"\")\n",
    "#                     typ = im.get(\"type\", \"output\")\n",
    "#                     # Save as ComfyUI_<originalname>.png in same folder\n",
    "#                     base = os.path.splitext(os.path.basename(frame_path))[0]\n",
    "#                     save_dir = os.path.dirname(frame_path)\n",
    "#                     out_path = os.path.join(save_dir, f\"ComfyUI_{base}.png\")\n",
    "#                     seg_path = download_image(fn, server=comfy_server,\n",
    "#                                               subfolder=sub, folder_type=typ,\n",
    "#                                               to_path=out_path)\n",
    "#                     break\n",
    "#         if seg_path:\n",
    "#             break\n",
    "#         time.sleep(0.5)\n",
    "\n",
    "#     if not seg_path:\n",
    "#         raise RuntimeError(f\"No outputs from ComfyUI for {frame_path}\")\n",
    "\n",
    "#     return seg_path\n",
    "\n",
    "\n",
    "# def run_sam_on_frame_multi(frame_path, comfy_server=COMFY):\n",
    "#     \"\"\"\n",
    "#     Send one frame through ComfyUI workflow and return 4 mask paths:\n",
    "#     clothes, arms, necks, faces.\n",
    "#     Then compute two final masks:\n",
    "#       1. clothes - (arms+faces+necks)\n",
    "#       2. (arms+necks)\n",
    "#     \"\"\"\n",
    "#     uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "#     # Load workflow JSON\n",
    "#     with open(WORKFLOW_JSON, \"r\") as f:\n",
    "#         prompt = json.load(f)\n",
    "\n",
    "#     # Patch LoadImage node\n",
    "#     if not patch_loadimage_node(prompt, uploaded):\n",
    "#         raise RuntimeError(\"Could not patch LoadImage node in workflow JSON.\")\n",
    "\n",
    "#     # Queue to Comfy\n",
    "#     prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "#     deadline = time.time() + 600\n",
    "#     outputs = {}\n",
    "\n",
    "#     # Poll until we get outputs\n",
    "#     while time.time() < deadline:\n",
    "#         hist = get_history(prompt_id, server=comfy_server)\n",
    "#         item = hist.get(prompt_id)\n",
    "#         if item and \"outputs\" in item:\n",
    "#             for node_id, node_out in item[\"outputs\"].items():\n",
    "#                 for im in node_out.get(\"images\", []):\n",
    "#                     fn = im[\"filename\"]\n",
    "#                     sub = im.get(\"subfolder\", \"\")\n",
    "#                     typ = im.get(\"type\", \"output\")\n",
    "\n",
    "#                     save_dir = os.path.dirname(frame_path)\n",
    "#                     label = None\n",
    "#                     if node_id == \"12\": label = \"clothes\"\n",
    "#                     elif node_id == \"13\": label = \"arms\"\n",
    "#                     elif node_id == \"14\": label = \"necks\"\n",
    "#                     elif node_id == \"15\": label = \"faces\"\n",
    "#                     else: continue\n",
    "\n",
    "#                     out_path = os.path.join(save_dir, f\"{label}_{os.path.basename(frame_path)}\")\n",
    "#                     outputs[label] = download_image(fn, server=comfy_server,\n",
    "#                                                     subfolder=sub, folder_type=typ,\n",
    "#                                                     to_path=out_path)\n",
    "#         if len(outputs) == 4:\n",
    "#             break\n",
    "#         time.sleep(0.5)\n",
    "\n",
    "#     if len(outputs) < 4:\n",
    "#         raise RuntimeError(f\"Missing outputs from ComfyUI for {frame_path}: {outputs.keys()}\")\n",
    "\n",
    "#     # --- Build final composite masks ---\n",
    "#     clothes = cv2.imread(outputs[\"clothes\"], cv2.IMREAD_GRAYSCALE)\n",
    "#     arms   = cv2.imread(outputs[\"arms\"], cv2.IMREAD_GRAYSCALE)\n",
    "#     necks  = cv2.imread(outputs[\"necks\"], cv2.IMREAD_GRAYSCALE)\n",
    "#     faces  = cv2.imread(outputs[\"faces\"], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#     union_body = cv2.bitwise_or(arms, necks)\n",
    "#     union_body_face = cv2.bitwise_or(union_body, faces)\n",
    "\n",
    "#     clothes_final = cv2.subtract(clothes, union_body_face)    # Clothes minus (arms+faces+necks)\n",
    "#     body_final = union_body                                   # Arms+necks\n",
    "\n",
    "#     # Save them\n",
    "#     clothes_path = os.path.join(save_dir, f\"ClothesFinal_{os.path.basename(frame_path)}\")\n",
    "#     body_path    = os.path.join(save_dir, f\"BodyFinal_{os.path.basename(frame_path)}\")\n",
    "#     cv2.imwrite(clothes_path, clothes_final)\n",
    "#     cv2.imwrite(body_path, body_final)\n",
    "\n",
    "#     outputs[\"clothes_final\"] = clothes_path\n",
    "#     outputs[\"body_final\"] = body_path\n",
    "\n",
    "#     return outputs\n",
    "\n",
    "\n",
    "\n",
    "# # ---- Stage 3: SAM (extract → ComfyUI per frame → video) ----\n",
    "# def run_sam(input_path, out_dir, name):\n",
    "#     sam_frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "#     os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "#     sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "\n",
    "#     if os.path.exists(sam_path):\n",
    "#         print(f\"[CACHE] Using cached SAM video: {sam_path}\")\n",
    "#         return sam_path\n",
    "\n",
    "#     # --- Step 1: Extract all frames first ---\n",
    "#     cap = cv2.VideoCapture(input_path)\n",
    "#     fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "#     width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     idx = 0\n",
    "#     with tqdm(total=total_frames, desc=\"Extracting frames\", unit=\"frame\") as pbar:\n",
    "#         while True:\n",
    "#             ret, frame_bgr = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame_file = os.path.join(sam_frames_dir, f\"frame_{idx:06d}.png\")\n",
    "#             if not os.path.exists(frame_file):\n",
    "#                 cv2.imwrite(frame_file, frame_bgr)\n",
    "#             idx += 1\n",
    "#             pbar.update(1)\n",
    "#     cap.release()\n",
    "\n",
    "#     # --- Step 2: Run ComfyUI on each frame ---\n",
    "#     frame_files = sorted([f for f in os.listdir(sam_frames_dir) if f.startswith(\"frame_\")])\n",
    "#     for fname in tqdm(frame_files, desc=\"Processing with ComfyUI\", unit=\"frame\"):\n",
    "#         frame_file = os.path.join(sam_frames_dir, fname)\n",
    "#         out_file = os.path.join(sam_frames_dir, f\"ComfyUI_{fname}\")\n",
    "#         if os.path.exists(out_file):\n",
    "#             continue\n",
    "#         try:\n",
    "#             run_sam_on_frame(frame_file, comfy_server=COMFY)\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ SAM failed on {fname}: {e}\")\n",
    "\n",
    "#     # --- Step 3: Collect only ComfyUI_* frames ---\n",
    "#     sam_files = sorted([f for f in os.listdir(sam_frames_dir) if f.startswith(\"ComfyUI_\")])\n",
    "\n",
    "#     # --- Step 4: Combine into video ---\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "#     writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "#     for seg_file in tqdm(sam_files, desc=\"Building SAM video\", unit=\"frame\"):\n",
    "#         img = cv2.imread(os.path.join(sam_frames_dir, seg_file))\n",
    "#         if img is None:\n",
    "#             continue\n",
    "#         img_resized = cv2.resize(img, (width, height))\n",
    "#         writer.write(img_resized)\n",
    "\n",
    "#     writer.release()\n",
    "#     print(f\"[INFO] SAM video saved: {sam_path}\")\n",
    "#     return sam_path\n",
    "\n",
    "\n",
    "\n",
    "# def run_sam_multi(input_path, out_dir, name):\n",
    "#     \"\"\"\n",
    "#     Runs ComfyUI workflow on all frames and produces 2 final mask videos:\n",
    "#       - ClothesFinal video\n",
    "#       - BodyFinal video\n",
    "#     \"\"\"\n",
    "#     frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "#     os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "#     clothes_path = os.path.join(out_dir, f\"{name}_clothesFinal.mp4\")\n",
    "#     body_path    = os.path.join(out_dir, f\"{name}_bodyFinal.mp4\")\n",
    "\n",
    "#     if os.path.exists(clothes_path) and os.path.exists(body_path):\n",
    "#         print(f\"[CACHE] Using cached SAM videos: {clothes_path}, {body_path}\")\n",
    "#         return clothes_path, body_path\n",
    "\n",
    "#     cap = cv2.VideoCapture(input_path)\n",
    "#     fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "#     width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     # Extract frames\n",
    "#     frame_files = []\n",
    "#     idx = 0\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret: break\n",
    "#         fpath = os.path.join(frames_dir, f\"frame_{idx:06d}.png\")\n",
    "#         cv2.imwrite(fpath, frame)\n",
    "#         frame_files.append(fpath)\n",
    "#         idx += 1\n",
    "#     cap.release()\n",
    "\n",
    "#     clothes_writer = cv2.VideoWriter(clothes_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "#     body_writer    = cv2.VideoWriter(body_path,    cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "#     for f in tqdm(frame_files, desc=\"Comfy Multi-Mask\", unit=\"frame\"):\n",
    "#         try:\n",
    "#             outs = run_sam_on_frame_multi(f, comfy_server=COMFY)\n",
    "\n",
    "#             clothes = cv2.imread(outs[\"clothes_final\"], cv2.IMREAD_GRAYSCALE)\n",
    "#             body    = cv2.imread(outs[\"body_final\"], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#             # Resize to match original video dimensions\n",
    "#             clothes = cv2.resize(clothes, (width, height))\n",
    "#             body    = cv2.resize(body, (width, height))\n",
    "\n",
    "#             # Binarize masks\n",
    "#             _, clothes = cv2.threshold(clothes, 127, 255, cv2.THRESH_BINARY)\n",
    "#             _, body    = cv2.threshold(body, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#             # Convert to 3-channel (so OpenCV writes consistent frames)\n",
    "#             clothes_color = cv2.cvtColor(clothes, cv2.COLOR_GRAY2BGR)\n",
    "#             body_color    = cv2.cvtColor(body, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "#             clothes_writer.write(clothes_color)\n",
    "#             body_writer.write(body_color)\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ Failed on {f}: {e}\")\n",
    "\n",
    "#     clothes_writer.release()\n",
    "#     body_writer.release()\n",
    "\n",
    "#     print(f\"[INFO] SAM multi videos saved: {clothes_path}, {body_path}\")\n",
    "#     return clothes_path, body_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ---- Stage 1: YOLO ----\n",
    "# def run_yolo(input_path, out_dir, name):\n",
    "#     yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "#     results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "#     if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "#         print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "#         with open(results_path, \"rb\") as f:\n",
    "#             results_per_frame = pickle.load(f)\n",
    "#         return yolo_path, results_per_frame\n",
    "\n",
    "#     cap = cv2.VideoCapture(input_path)\n",
    "#     fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "#     width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "#     writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "#     results_per_frame = []\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "#             writer.write(results[0].plot())\n",
    "#             results_per_frame.append({\n",
    "#                 \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "#                 \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "#                 \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "#             })\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     cap.release()\n",
    "#     writer.release()\n",
    "\n",
    "#     with open(results_path, \"wb\") as f:\n",
    "#         pickle.dump(results_per_frame, f)\n",
    "\n",
    "#     return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# # ---- Stage 2: DeOldify ----\n",
    "# def run_deoldify(input_path, out_dir, name):\n",
    "#     deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "#     if os.path.exists(deoldify_path):\n",
    "#         print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "#         return deoldify_path\n",
    "\n",
    "#     cap = cv2.VideoCapture(input_path)\n",
    "#     fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "#     width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "#     writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "#         while True:\n",
    "#             ret, frame_bgr = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "#             deold = deoldify_inference(frame_rgb)\n",
    "#             writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "#             pbar.update(1)\n",
    "#     cap.release()\n",
    "#     writer.release()\n",
    "#     return deoldify_path\n",
    "\n",
    "\n",
    "# # ---- Stage 3: SAM (with frame folder + resume support) ----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ---- Stage 4: Fusion ----\n",
    "# def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "#     fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "#     if os.path.exists(fusion_path):\n",
    "#         print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "#         return fusion_path\n",
    "\n",
    "#     cap_input = cv2.VideoCapture(input_path)      # original frames\n",
    "#     cap_deold = cv2.VideoCapture(deoldify_path)   # deoldify video\n",
    "#     cap_sam = cv2.VideoCapture(sam_path)          # sam masks\n",
    "\n",
    "#     fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "#     width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "#     writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "#     total_frames = int(min(\n",
    "#         len(yolo_results),\n",
    "#         cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "#         cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "#         cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "#     ))\n",
    "\n",
    "#     frame_idx = 0\n",
    "#     with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "#         while frame_idx < total_frames:\n",
    "#             ret_in, frame_in = cap_input.read()\n",
    "#             ret_deold, frame_deold = cap_deold.read()\n",
    "#             ret_sam, frame_sam = cap_sam.read()\n",
    "#             if not (ret_in and ret_deold and ret_sam):\n",
    "#                 break\n",
    "\n",
    "#             # SAM mask\n",
    "#             gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "#             _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#             # YOLO mask\n",
    "#             yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "#             for box, conf, cls in zip(\n",
    "#                 yolo_results[frame_idx][\"boxes\"],\n",
    "#                 yolo_results[frame_idx][\"conf\"],\n",
    "#                 yolo_results[frame_idx][\"cls\"]\n",
    "#             ):\n",
    "#                 if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "#                     continue\n",
    "#                 x1, y1, x2, y2 = map(int, box)\n",
    "#                 yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "#             # Intersection\n",
    "#             intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "#             mask_bool = intersect > 127\n",
    "\n",
    "#             # Fusion: base is ORIGINAL frame\n",
    "#             fusion_frame = frame_in.copy()\n",
    "#             fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "#             writer.write(fusion_frame)\n",
    "#             frame_idx += 1\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     cap_input.release()\n",
    "#     cap_deold.release()\n",
    "#     cap_sam.release()\n",
    "#     writer.release()\n",
    "\n",
    "#     print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "#     return fusion_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def run_fusion_single(input_path, out_dir, name, yolo_results, deoldify_path, mask_path, label):\n",
    "#     \"\"\"\n",
    "#     Fuse one mask (either ClothesFinal or BodyFinal) with deoldify result,\n",
    "#     applying the SAME logic as run_fusion: intersection of YOLO and SAM masks.\n",
    "#     \"\"\"\n",
    "#     fusion_path = os.path.join(out_dir, f\"{name}_{label}_final.mp4\")\n",
    "#     if os.path.exists(fusion_path):\n",
    "#         print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "#         return fusion_path\n",
    "\n",
    "#     cap_input = cv2.VideoCapture(input_path)\n",
    "#     cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "#     cap_mask  = cv2.VideoCapture(mask_path)\n",
    "\n",
    "#     fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "#     width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#     writer = cv2.VideoWriter(fusion_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "#     total_frames = min(int(cap_input.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "#                        int(cap_deold.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "#                        int(cap_mask.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "#                        len(yolo_results))\n",
    "\n",
    "#     frame_idx = 0\n",
    "#     for _ in tqdm(range(total_frames), desc=f\"Fusion-{label}\", unit=\"frame\"):\n",
    "#         ret_in, frame_in = cap_input.read()\n",
    "#         ret_de, frame_de = cap_deold.read()\n",
    "#         ret_m, mask_img  = cap_mask.read()\n",
    "#         if not (ret_in and ret_de and ret_m): break\n",
    "\n",
    "#         # --- SAM mask (from clothes/body video) ---\n",
    "#         sam_gray = cv2.cvtColor(mask_img, cv2.COLOR_BGR2GRAY)\n",
    "#         _, sam_mask = cv2.threshold(sam_gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#         # --- YOLO mask (same logic as run_fusion) ---\n",
    "#         yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "#         for box, conf, cls in zip(\n",
    "#             yolo_results[frame_idx][\"boxes\"],\n",
    "#             yolo_results[frame_idx][\"conf\"],\n",
    "#             yolo_results[frame_idx][\"cls\"]\n",
    "#         ):\n",
    "#             if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "#                 continue\n",
    "#             x1, y1, x2, y2 = map(int, box)\n",
    "#             x1, y1 = max(0, x1), max(0, y1)\n",
    "#             x2, y2 = min(width, x2), min(height, y2)\n",
    "#             yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "#         # --- Intersection YOLO ∩ SAM ---\n",
    "#         intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "#         mask_bool = intersect > 127\n",
    "\n",
    "#         # --- Apply fusion (overlay DeOldify where mask == 1) ---\n",
    "#         fusion_frame = frame_in.copy()\n",
    "#         fusion_frame[mask_bool] = frame_de[mask_bool]\n",
    "\n",
    "#         writer.write(fusion_frame)\n",
    "#         frame_idx += 1\n",
    "\n",
    "#     cap_input.release(); cap_deold.release(); cap_mask.release(); writer.release()\n",
    "#     print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "#     return fusion_path\n",
    "\n",
    "\n",
    "\n",
    "# # ---- Main Pipeline ----\n",
    "# def process_video(input_path):\n",
    "#     folder, fname = os.path.split(input_path)\n",
    "#     name, _ = os.path.splitext(fname)\n",
    "#     out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "#     deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "#     sam_path = run_sam(input_path, out_dir, name)\n",
    "#     fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "#     print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "#     return {\n",
    "#         \"yolo\": yolo_path,\n",
    "#         \"deoldify\": deoldify_path,\n",
    "#         \"sam\": sam_path,\n",
    "#         \"final\": fusion_path\n",
    "#     }\n",
    "\n",
    "\n",
    "# def process_video_multi(input_path):\n",
    "#     folder, fname = os.path.split(input_path)\n",
    "#     name, _ = os.path.splitext(fname)\n",
    "#     out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     # Stage 1: YOLO\n",
    "#     yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "\n",
    "#     # Stage 2: DeOldify\n",
    "#     deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "\n",
    "#     # Stage 3: SAM multi-mask\n",
    "#     clothes_mask_path, body_mask_path = run_sam_multi(input_path, out_dir, name)\n",
    "\n",
    "#     # Stage 4: Fusion (separate)\n",
    "#     clothes_final = run_fusion_single(input_path, out_dir, name, yolo_results, deoldify_path, clothes_mask_path, \"clothes\")\n",
    "#     body_final    = run_fusion_single(input_path, out_dir, name, yolo_results, deoldify_path, body_mask_path, \"body\")\n",
    "\n",
    "#     return {\n",
    "#         \"yolo\": yolo_path,\n",
    "#         \"deoldify\": deoldify_path,\n",
    "#         \"clothes_mask\": clothes_mask_path,\n",
    "#         \"body_mask\": body_mask_path,\n",
    "#         \"clothes_final\": clothes_final,\n",
    "#         \"body_final\": body_final\n",
    "#     }\n",
    "\n",
    "\n",
    "# def process_video_cached(input_path):\n",
    "#     \"\"\"\n",
    "#     Cached wrapper around process_video().\n",
    "#     Returns only the final fusion video path.\n",
    "#     \"\"\"\n",
    "#     folder, fname = os.path.split(input_path)\n",
    "#     name, _ = os.path.splitext(fname)\n",
    "#     out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "#     final_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "\n",
    "#     if os.path.exists(final_path):\n",
    "#         print(f\"[CACHE] Final output exists: {final_path}\")\n",
    "#         return final_path\n",
    "\n",
    "#     outputs = process_video(input_path)\n",
    "#     return outputs[\"final\"]\n",
    "\n",
    "\n",
    "# # # # ---- Example ----\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "# #     #outputs = process_video(input_video)\n",
    "# #     outputs = process_video_multi(input_video)\n",
    "# #     print(\"Pipeline outputs:\")\n",
    "# #     for k, v in outputs.items():\n",
    "# #         print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7976aa-d8a5-4052-861b-4010769994e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/deoldify/fastai/data_block.py:451: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\n",
      "  warn(\"Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\")\n",
      "/opt/deoldify/fastai/data_block.py:453: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  warn(\"\"\"Your validation set is empty. If this is by design, use `split_none()`\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/opt/deoldify/fastai/basic_train.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(tmp_file)\n",
      "/opt/deoldify/fastai/basic_train.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(source, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, torch, json, uuid, time, pickle, requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"\n",
    "WORKFLOW_JSON = \"ClothesArmsFaceNeck.json\"\n",
    "# =================\n",
    "\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- Helpers for DeOldify ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI Helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return to_path\n",
    "\n",
    "\n",
    "# ---- Run ComfyUI for multi-prompt masks ----\n",
    "\n",
    "\n",
    "def run_multi_masks(frame_path, comfy_server=COMFY):\n",
    "    \"\"\"\n",
    "    Runs the ComfyUI workflow and returns masks with semantic names\n",
    "    instead of numeric node IDs.\n",
    "    Returns dict: {\"clothes\": path, \"arms\": path, \"neck\": path, \"faces\": path}\n",
    "    \"\"\"\n",
    "    uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # Patch input image for all LoadImage nodes\n",
    "    for node in prompt.values():\n",
    "        if node.get(\"class_type\", \"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = uploaded\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 600\n",
    "    masks = {}\n",
    "\n",
    "    # Mapping from node_id to semantic name (adjust if your JSON changes)\n",
    "    id_to_name = {\n",
    "        \"2\": \"clothes\",\n",
    "        \"6\": \"arms\",\n",
    "        \"8\": \"neck\",\n",
    "        \"10\": \"faces\"\n",
    "    }\n",
    "\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_id, node_out in item[\"outputs\"].items():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "                    out_path = os.path.join(os.path.dirname(frame_path),\n",
    "                                            f\"{id_to_name.get(node_id, node_id)}_{os.path.basename(frame_path)}\")\n",
    "                    dl_path = download_image(fn, server=comfy_server, subfolder=sub,\n",
    "                                             folder_type=typ, to_path=out_path)\n",
    "\n",
    "                    if node_id in id_to_name:\n",
    "                        masks[id_to_name[node_id]] = dl_path\n",
    "                    else:\n",
    "                        masks[node_id] = dl_path  # fallback if unmapped\n",
    "        if masks:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return masks   # {\"clothes\": path, \"arms\": path, \"neck\": path, \"faces\": path}\n",
    "\n",
    "\n",
    "# ---- Stage SAM + Mask Fusion ----\n",
    "def run_sam_and_fuse(input_path, out_dir, name):\n",
    "    sam_frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    idx = 0\n",
    "    finalA_path = os.path.join(out_dir, f\"{name}_maskA.mp4\")  # clothes+faces+arms+neck\n",
    "    finalB_path = os.path.join(out_dir, f\"{name}_maskB.mp4\")  # arms+neck\n",
    "\n",
    "    writerA = cv2.VideoWriter(finalA_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "    writerB = cv2.VideoWriter(finalB_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"SAM+Fusion\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_file = os.path.join(sam_frames_dir, f\"frame_{idx:06d}.png\")\n",
    "            cv2.imwrite(frame_file, frame_bgr)\n",
    "\n",
    "            try:\n",
    "                masks = run_multi_masks(frame_file, comfy_server=COMFY)\n",
    "                # Load each mask\n",
    "                mask_imgs = {}\n",
    "                for k, v in masks.items():\n",
    "                    img = cv2.imread(v, cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is not None:\n",
    "                        mask_imgs[k] = cv2.resize(img, (width, height))\n",
    "\n",
    "                # Fuse masks\n",
    "                mask_clothes = mask_imgs.get(\"2\", np.zeros((height, width), np.uint8))\n",
    "                mask_arms    = mask_imgs.get(\"6\", np.zeros((height, width), np.uint8))\n",
    "                mask_neck    = mask_imgs.get(\"8\", np.zeros((height, width), np.uint8))\n",
    "                mask_faces   = mask_imgs.get(\"10\", np.zeros((height, width), np.uint8))\n",
    "\n",
    "                finalA = cv2.bitwise_or(mask_clothes,\n",
    "                         cv2.bitwise_or(mask_arms,\n",
    "                         cv2.bitwise_or(mask_neck, mask_faces)))\n",
    "                finalB = cv2.bitwise_or(mask_arms, mask_neck)\n",
    "\n",
    "                writerA.write(cv2.cvtColor(finalA, cv2.COLOR_GRAY2BGR))\n",
    "                writerB.write(cv2.cvtColor(finalB, cv2.COLOR_GRAY2BGR))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Mask fusion failed on frame {idx}: {e}\")\n",
    "\n",
    "            idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writerA.release()\n",
    "    writerB.release()\n",
    "\n",
    "    return finalA_path, finalB_path\n",
    "\n",
    "\n",
    "# ---- Fusion with DeOldify ----\n",
    "def run_fusion_with_masks(input_path, deoldify_path, mask_path, out_path):\n",
    "    cap_in = cv2.VideoCapture(input_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_mask = cv2.VideoCapture(mask_path)\n",
    "\n",
    "    fps = int(cap_in.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_in.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_in.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    while True:\n",
    "        ret_in, f_in = cap_in.read()\n",
    "        ret_de, f_de = cap_deold.read()\n",
    "        ret_m, f_m = cap_mask.read()\n",
    "        if not (ret_in and ret_de and ret_m):\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(f_m, cv2.COLOR_BGR2GRAY)\n",
    "        _, mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "        mask_bool = mask > 127\n",
    "\n",
    "        fused = f_in.copy()\n",
    "        fused[mask_bool] = f_de[mask_bool]\n",
    "        writer.write(fused)\n",
    "\n",
    "    cap_in.release()\n",
    "    cap_deold.release()\n",
    "    cap_mask.release()\n",
    "    writer.release()\n",
    "\n",
    "\n",
    "# ---- Main ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: DeOldify entire video\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if not os.path.exists(deoldify_path):\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        writer = cv2.VideoWriter(deoldify_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "        cap.release(); writer.release()\n",
    "\n",
    "    # Step 2: Run SAM + fuse masks\n",
    "    maskA_path, maskB_path = run_sam_and_fuse(input_path, out_dir, name)\n",
    "\n",
    "    # Step 3: Apply fusion twice\n",
    "    finalA = os.path.join(out_dir, f\"{name}_finalA.mp4\")\n",
    "    finalB = os.path.join(out_dir, f\"{name}_finalB.mp4\")\n",
    "\n",
    "    run_fusion_with_masks(input_path, deoldify_path, maskA_path, finalA)\n",
    "    run_fusion_with_masks(input_path, deoldify_path, maskB_path, finalB)\n",
    "\n",
    "    print(f\"[INFO] Final outputs saved:\\n - {finalA}\\n - {finalB}\")\n",
    "    return finalA, finalB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # # ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "#     #outputs = process_video(input_video)\n",
    "#     outputs = process_video_multi(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k, v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f9b52-87ff-4ebb-badc-6b751f0f75f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image without yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408ec946-1671-4421-a902-5def4be9ef4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outputs written to outputs/first_frame\n",
      "Pipeline outputs:\n",
      " - deoldify: outputs/first_frame/first_frame_deoldify.png\n",
      " - raw_masks: ['first_frame_raw_mask_arms.png', 'first_frame_raw_mask_neck.png', 'first_frame_raw_mask_clothes.png', 'first_frame_raw_mask_faces.png']\n",
      " - bin_masks: ['first_frame_bin_mask_arms.png', 'first_frame_bin_mask_clothes.png', 'first_frame_bin_mask_neck.png', 'first_frame_bin_mask_faces.png']\n",
      " - maskA: outputs/first_frame/first_frame_maskA_exclusive_clothes.png\n",
      " - maskB: outputs/first_frame/first_frame_maskB_arms_neck.png\n",
      " - finalA: outputs/first_frame/first_frame_finalA_exclusive_clothes.png\n",
      " - finalB: outputs/first_frame/first_frame_finalB_arms_neck.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def ensure_binary(img, size):\n",
    "    \"\"\"Resize + convert grayscale mask to binary 0/255.\"\"\"\n",
    "    if img is None:\n",
    "        return np.zeros(size, dtype=np.uint8)\n",
    "    img = cv2.resize(img, size)\n",
    "    _, bin_img = cv2.threshold(img, 1, 255, cv2.THRESH_BINARY)\n",
    "    return bin_img\n",
    "\n",
    "def process_image(input_image):\n",
    "    folder, fname = os.path.split(input_image)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ---- Load frame ----\n",
    "    frame_bgr = cv2.imread(input_image)\n",
    "    if frame_bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not load image {input_image}\")\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "\n",
    "    # ---- Stage 1: DeOldify ----\n",
    "    deold = deoldify_inference(frame_rgb)\n",
    "    deold_bgr = cv2.cvtColor(deold, cv2.COLOR_RGB2BGR)\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.png\")\n",
    "    cv2.imwrite(deoldify_path, deold_bgr)\n",
    "\n",
    "    # ---- Stage 2: SAM (multi-mask via ComfyUI workflow) ----\n",
    "    try:\n",
    "        masks = run_multi_masks(input_image, comfy_server=COMFY)  # dict {node_id: path}\n",
    "\n",
    "        # Map node IDs → semantic names\n",
    "        id_to_name = {\n",
    "            \"12\": \"clothes\",\n",
    "            \"13\": \"arms\",\n",
    "            \"14\": \"neck\",\n",
    "            \"15\": \"faces\"\n",
    "        }\n",
    "\n",
    "        mask_imgs = {}\n",
    "        for k, v in masks.items():\n",
    "            semantic_name = id_to_name.get(k, k)  # fallback to original ID if not mapped\n",
    "            img = cv2.imread(v, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                # Save raw mask\n",
    "                cv2.imwrite(os.path.join(out_dir, f\"{name}_raw_mask_{semantic_name}.png\"), img)\n",
    "\n",
    "                # Convert to binary + resize\n",
    "                mask_bin = ensure_binary(img, (w, h))\n",
    "                mask_imgs[semantic_name] = mask_bin\n",
    "\n",
    "                # Save binary mask\n",
    "                cv2.imwrite(os.path.join(out_dir, f\"{name}_bin_mask_{semantic_name}.png\"), mask_bin)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ SAM failed on image: {e}\")\n",
    "        mask_imgs = {}\n",
    "\n",
    "    # ---- Stage 3: Build composites ----\n",
    "    mask_clothes = mask_imgs.get(\"clothes\", np.zeros((h, w), np.uint8))\n",
    "    mask_arms    = mask_imgs.get(\"arms\", np.zeros((h, w), np.uint8))\n",
    "    mask_neck    = mask_imgs.get(\"neck\", np.zeros((h, w), np.uint8))\n",
    "    mask_faces   = mask_imgs.get(\"faces\", np.zeros((h, w), np.uint8))\n",
    "\n",
    "    # Exclusive clothes = clothes - (arms ∪ neck ∪ faces)\n",
    "    others = cv2.bitwise_or(mask_arms, cv2.bitwise_or(mask_neck, mask_faces))\n",
    "    maskA = cv2.bitwise_and(mask_clothes, cv2.bitwise_not(others))\n",
    "\n",
    "    # Arms + neck\n",
    "    maskB = cv2.bitwise_or(mask_arms, mask_neck)\n",
    "\n",
    "    # Save composite masks\n",
    "    maskA_path = os.path.join(out_dir, f\"{name}_maskA_exclusive_clothes.png\")\n",
    "    maskB_path = os.path.join(out_dir, f\"{name}_maskB_arms_neck.png\")\n",
    "    cv2.imwrite(maskA_path, maskA)\n",
    "    cv2.imwrite(maskB_path, maskB)\n",
    "\n",
    "    # ---- Stage 4: Fusion with DeOldify ----\n",
    "    maskA_bool = maskA > 127\n",
    "    maskB_bool = maskB > 127\n",
    "\n",
    "    fusionA = frame_bgr.copy()\n",
    "    fusionA[maskA_bool] = deold_bgr[maskA_bool]\n",
    "\n",
    "    fusionB = frame_bgr.copy()\n",
    "    fusionB[maskB_bool] = deold_bgr[maskB_bool]\n",
    "\n",
    "    fusionA_path = os.path.join(out_dir, f\"{name}_finalA_exclusive_clothes.png\")\n",
    "    fusionB_path = os.path.join(out_dir, f\"{name}_finalB_arms_neck.png\")\n",
    "    cv2.imwrite(fusionA_path, fusionA)\n",
    "    cv2.imwrite(fusionB_path, fusionB)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"raw_masks\": [f for f in os.listdir(out_dir) if f.startswith(f\"{name}_raw_mask_\")],\n",
    "        \"bin_masks\": [f for f in os.listdir(out_dir) if f.startswith(f\"{name}_bin_mask_\")],\n",
    "        \"maskA\": maskA_path,\n",
    "        \"maskB\": maskB_path,\n",
    "        \"finalA\": fusionA_path,\n",
    "        \"finalB\": fusionB_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_image = \"input_videos/first_frame.jpg\"\n",
    "    outputs = process_image(input_image)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1436b-80b2-4670-9c83-1230732cc828",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "video with yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e647265b-7647-4810-bfc3-43f55ed693b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Using: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/deoldify/fastai/data_block.py:451: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\n",
      "  warn(\"Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\")\n",
      "/opt/deoldify/fastai/data_block.py:453: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  warn(\"\"\"Your validation set is empty. If this is by design, use `split_none()`\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/opt/deoldify/fastai/basic_train.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(tmp_file)\n",
      "/opt/deoldify/fastai/basic_train.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(source, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "\n",
    "# ---- NEW CONFIG ----\n",
    "BBOX_ENLARGE = 0.2       # enlarge bbox by 20%\n",
    "TOP_K_BBOX = 3           # number of top boxes per frame to run SAM\n",
    "GDINO_PROMPT = \"clothes\" # grounding dino prompt\n",
    "GDINO_THRESHOLD = 0.30   # grounding dino threshold\n",
    "# =================\n",
    "\n",
    "\n",
    "def patch_groundingdino_node(prompt_dict, new_prompt=None, new_threshold=None):\n",
    "    \"\"\"Patch GroundingDinoSAMSegment node with new prompt/threshold values.\"\"\"\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\", \"\").lower().startswith(\"groundingdinosamsegment\"):\n",
    "            if new_prompt is not None:\n",
    "                node[\"inputs\"][\"prompt\"] = new_prompt\n",
    "            if new_threshold is not None:\n",
    "                node[\"inputs\"][\"threshold\"] = new_threshold\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    return to_path\n",
    "\n",
    "\n",
    "def run_sam_on_frame(frame_path, comfy_server=COMFY):\n",
    "    \"\"\"Send one frame (crop) through ComfyUI workflow and return saved mask path.\"\"\"\n",
    "    uploaded = upload_image_to_comfy(frame_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "\n",
    "    # Patch LoadImage node\n",
    "    if not patch_loadimage_node(prompt, uploaded):\n",
    "        raise RuntimeError(\"Could not patch LoadImage node in workflow JSON.\")\n",
    "\n",
    "    # 🔹 Patch GroundingDino node with dynamic prompt & threshold\n",
    "    patch_groundingdino_node(prompt, new_prompt=GDINO_PROMPT, new_threshold=GDINO_THRESHOLD)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 600\n",
    "    seg_path = None\n",
    "\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    fn = im[\"filename\"]\n",
    "                    sub = im.get(\"subfolder\", \"\")\n",
    "                    typ = im.get(\"type\", \"output\")\n",
    "                    base = os.path.splitext(os.path.basename(frame_path))[0]\n",
    "                    save_dir = os.path.dirname(frame_path)\n",
    "                    out_path = os.path.join(save_dir, f\"ComfyUI_{base}.png\")\n",
    "                    seg_path = download_image(fn, server=comfy_server,\n",
    "                                              subfolder=sub, folder_type=typ,\n",
    "                                              to_path=out_path)\n",
    "                    break\n",
    "        if seg_path:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if not seg_path:\n",
    "        raise RuntimeError(f\"No outputs from ComfyUI for {frame_path}\")\n",
    "\n",
    "    return seg_path\n",
    "\n",
    "# ---- Stage 3: SAM with YOLO BBoxes ----\n",
    "def run_sam(input_path, out_dir, name, yolo_results):\n",
    "    sam_frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM video: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"SAM with YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            results = yolo_results[frame_idx]\n",
    "            boxes, confs, clses = results[\"boxes\"], results[\"conf\"], results[\"cls\"]\n",
    "\n",
    "            order = np.argsort(confs)[::-1][:TOP_K_BBOX]\n",
    "            masks_for_frame = []\n",
    "\n",
    "            for i in order:\n",
    "                if int(clses[i]) != 0 or confs[i] < CONF_THRESHOLD:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, boxes[i])\n",
    "\n",
    "                # enlarge box\n",
    "                bw = x2 - x1\n",
    "                bh = y2 - y1\n",
    "                x1 = max(0, int(x1 - BBOX_ENLARGE * bw))\n",
    "                y1 = max(0, int(y1 - BBOX_ENLARGE * bh))\n",
    "                x2 = min(width, int(x2 + BBOX_ENLARGE * bw))\n",
    "                y2 = min(height, int(y2 + BBOX_ENLARGE * bh))\n",
    "\n",
    "                crop = frame_bgr[y1:y2, x1:x2]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "\n",
    "                crop_path = os.path.join(sam_frames_dir, f\"frame_{frame_idx:06d}_box{i}.png\")\n",
    "                cv2.imwrite(crop_path, crop)\n",
    "\n",
    "                try:\n",
    "                    seg_path = run_sam_on_frame(crop_path, comfy_server=COMFY)\n",
    "                    seg_img = cv2.imread(seg_path)\n",
    "                    seg_resized = cv2.resize(seg_img, (x2 - x1, y2 - y1))\n",
    "                    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "                    mask[y1:y2, x1:x2] = cv2.cvtColor(seg_resized, cv2.COLOR_BGR2GRAY)\n",
    "                    masks_for_frame.append(mask)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ SAM failed on frame {frame_idx}, box {i}: {e}\")\n",
    "                finally:\n",
    "                    # cleanup crop + box-level SAM output\n",
    "                    if os.path.exists(crop_path):\n",
    "                        os.remove(crop_path)\n",
    "                    box_seg = crop_path.replace(\".png\", \"\").replace(\"frame_\", \"ComfyUI_frame_\") + \".png\"\n",
    "                    if os.path.exists(box_seg):\n",
    "                        os.remove(box_seg)\n",
    "\n",
    "            # always save a mask (blank if no detections)\n",
    "            if masks_for_frame:\n",
    "                final_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "                for m in masks_for_frame:\n",
    "                    final_mask = cv2.bitwise_or(final_mask, m)\n",
    "            else:\n",
    "                final_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "            out_path = os.path.join(sam_frames_dir, f\"ComfyUI_frame_{frame_idx:06d}.png\")\n",
    "            cv2.imwrite(out_path, final_mask)\n",
    "\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "\n",
    "    # build video ONLY from final per-frame masks\n",
    "    sam_files = sorted([\n",
    "        f for f in os.listdir(sam_frames_dir)\n",
    "        if f.startswith(\"ComfyUI_frame_\") and \"_box\" not in f\n",
    "    ])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "    for seg_file in tqdm(sam_files, desc=\"Building SAM video\", unit=\"frame\"):\n",
    "        img = cv2.imread(os.path.join(sam_frames_dir, seg_file))\n",
    "        writer.write(cv2.resize(img, (width, height)))\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "            mask_bool = sam_mask > 127\n",
    "\n",
    "            fusion_frame = frame_in.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name, yolo_results)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def process_video_cached(input_path):\n",
    "    \"\"\"\n",
    "    Cached wrapper around process_video().\n",
    "    Returns only the final fusion video path.\n",
    "    \"\"\"\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    final_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"[CACHE] Final output exists: {final_path}\")\n",
    "        return final_path\n",
    "\n",
    "    outputs = process_video(input_path)\n",
    "    return outputs[\"final\"]\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "#     outputs = process_video(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k, v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a3e7475-401d-413e-858e-3225260ad589",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4134125518.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimage with yolo clipping\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "image with yolo clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48cb6f03-9449-4981-bc97-5bcc5444408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outputs written to outputs/first_frame\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/first_frame/first_frame_yolo.png\n",
      " - deoldify: outputs/first_frame/first_frame_deoldify.png\n",
      " - sam: outputs/first_frame/first_frame_sam.png\n",
      " - final: outputs/first_frame/first_frame_final.png\n"
     ]
    }
   ],
   "source": [
    "GDINO_PROMPT = \"clothes\" # grounding dino prompt\n",
    "GDINO_THRESHOLD = 0.30   # grounding dino threshold\n",
    "\n",
    "\n",
    "def process_image(input_image):\n",
    "    folder, fname = os.path.split(input_image)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ---- Load frame ----\n",
    "    frame_bgr = cv2.imread(input_image)\n",
    "    if frame_bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not load image {input_image}\")\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w = frame_bgr.shape[:2]\n",
    "\n",
    "    # ---- Stage 1: YOLO ----\n",
    "    yolo_results = yolo_model.predict(frame_bgr, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "    yolo_frame = yolo_results[0].plot()\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.png\")\n",
    "    cv2.imwrite(yolo_path, yolo_frame)\n",
    "\n",
    "    results_dict = {\n",
    "        \"boxes\": yolo_results[0].boxes.xyxy.cpu().numpy(),\n",
    "        \"conf\": yolo_results[0].boxes.conf.cpu().numpy(),\n",
    "        \"cls\": yolo_results[0].boxes.cls.cpu().numpy()\n",
    "    }\n",
    "    with open(os.path.join(out_dir, f\"{name}_yolo_results.pkl\"), \"wb\") as f:\n",
    "        pickle.dump([results_dict], f)\n",
    "\n",
    "    # ---- Stage 2: DeOldify ----\n",
    "    deold = deoldify_inference(frame_rgb)\n",
    "    deold_bgr = cv2.cvtColor(deold, cv2.COLOR_RGB2BGR)\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.png\")\n",
    "    cv2.imwrite(deoldify_path, deold_bgr)\n",
    "\n",
    "    # ---- Stage 3: SAM ----\n",
    "    sam_frames_dir = os.path.join(out_dir, f\"{name}_sam\")\n",
    "    os.makedirs(sam_frames_dir, exist_ok=True)\n",
    "\n",
    "    boxes, confs, clses = results_dict[\"boxes\"], results_dict[\"conf\"], results_dict[\"cls\"]\n",
    "    order = np.argsort(confs)[::-1][:TOP_K_BBOX]\n",
    "    masks_for_frame = []\n",
    "\n",
    "    for i in order:\n",
    "        if int(clses[i]) != 0 or confs[i] < CONF_THRESHOLD:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, boxes[i])\n",
    "        bw, bh = x2 - x1, y2 - y1\n",
    "        x1 = max(0, int(x1 - BBOX_ENLARGE * bw))\n",
    "        y1 = max(0, int(y1 - BBOX_ENLARGE * bh))\n",
    "        x2 = min(w, int(x2 + BBOX_ENLARGE * bw))\n",
    "        y2 = min(h, int(y2 + BBOX_ENLARGE * bh))\n",
    "\n",
    "        crop = frame_bgr[y1:y2, x1:x2]\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        crop_path = os.path.join(sam_frames_dir, f\"{name}_box{i}.png\")\n",
    "        cv2.imwrite(crop_path, crop)\n",
    "\n",
    "        try:\n",
    "            seg_path = run_sam_on_frame(crop_path, comfy_server=COMFY)\n",
    "            seg_img = cv2.imread(seg_path)\n",
    "            seg_resized = cv2.resize(seg_img, (x2 - x1, y2 - y1))\n",
    "            mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            mask[y1:y2, x1:x2] = cv2.cvtColor(seg_resized, cv2.COLOR_BGR2GRAY)\n",
    "            masks_for_frame.append(mask)\n",
    "        except Exception as e:\n",
    "            print(f\"SAM failed on box {i}: {e}\")\n",
    "\n",
    "    # Save final mask (blank if none)\n",
    "    if masks_for_frame:\n",
    "        final_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        for m in masks_for_frame:\n",
    "            final_mask = cv2.bitwise_or(final_mask, m)\n",
    "    else:\n",
    "        final_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.png\")\n",
    "    cv2.imwrite(sam_path, final_mask)\n",
    "\n",
    "    # ---- Stage 4: Fusion (exact same logic as run_fusion) ----\n",
    "    gray = final_mask.copy()\n",
    "    _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    mask_bool = sam_mask > 127\n",
    "\n",
    "    fusion_frame = frame_bgr.copy()\n",
    "    fusion_frame[mask_bool] = deold_bgr[mask_bool]\n",
    "\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.png\")\n",
    "    cv2.imwrite(fusion_path, fusion_frame)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_image = r\"input_videos/first_frame.jpg\"\n",
    "    outputs = process_image(input_image)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cb7b3-bca5-4b1e-aa70-a31949437d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4364a3-5528-4370-a0a4-509e13576aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2065c-50c7-4359-bf20-b011a278daa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2cc53-f385-4dbd-9bc8-bacc48cc4238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1366641-c52d-428e-8f6b-942a24f45364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd3d16-cdb1-4ea6-ab2d-78b0203d6f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29000e-3ca4-4d94-bebc-679b758a81d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921b31c-4321-4cc2-82d8-e6a911081323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca59a7-f4ef-4876-ab61-5b9ad95c7437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843f168-dce9-4086-b6a6-a7c363246d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2011deaa-ebce-49c6-8b6c-b86e7cb10fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d52a09-c9c7-4dd6-9aaf-56d6a7003f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f7519-8e0a-4007-80e3-ba121e41a155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8818909-473b-49f0-98de-72ee6f8b26a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94faf37-1c77-4b7e-8016-1602c9a44221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    return to_path\n",
    "\n",
    "def run_sam_on_frame(frame_rgb, comfy_server=COMFY):\n",
    "    tmp_path = f\"temp_frame_{uuid.uuid4().hex}.png\"\n",
    "    Image.fromarray(frame_rgb).save(tmp_path)\n",
    "    uploaded = upload_image_to_comfy(tmp_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_loadimage_node(prompt, uploaded)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 60\n",
    "    seg_path = None\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    seg_path = download_image(\n",
    "                        im[\"filename\"], server=comfy_server,\n",
    "                        subfolder=im.get(\"subfolder\", \"\"),\n",
    "                        folder_type=im.get(\"type\", \"output\")\n",
    "                    )\n",
    "                    break\n",
    "        if seg_path: break\n",
    "        time.sleep(0.5)\n",
    "    os.remove(tmp_path)\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    frames_dir = os.path.join(out_dir, f\"{name}_yolo_frames\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    results_per_frame = []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        for frame_idx in range(total_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\")\n",
    "\n",
    "            if os.path.exists(frame_path):\n",
    "                results_per_frame.append(None)  # results loaded separately if needed\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            plotted = results[0].plot()\n",
    "            cv2.imwrite(frame_path, plotted)\n",
    "\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "\n",
    "    # Save video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "    for frame_idx in range(total_frames):\n",
    "        img = cv2.imread(os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\"))\n",
    "        if img is not None:\n",
    "            writer.write(img)\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    frames_dir = os.path.join(out_dir, f\"{name}_deoldify_frames\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        for frame_idx in range(total_frames):\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\")\n",
    "            if os.path.exists(frame_path):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            cv2.imwrite(frame_path, cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "\n",
    "    # Save video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "    for frame_idx in range(total_frames):\n",
    "        img = cv2.imread(os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\"))\n",
    "        if img is not None:\n",
    "            writer.write(img)\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    frames_dir = os.path.join(out_dir, f\"{name}_sam_frames\")\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        for frame_idx in range(total_frames):\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\")\n",
    "            if os.path.exists(frame_path):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            cap = cv2.VideoCapture(input_path)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame_bgr = cap.read()\n",
    "            cap.release()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                seg_resized = cv2.resize(seg_img, (width, height)) if seg_img is not None else frame_bgr\n",
    "                cv2.imwrite(frame_path, seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ SAM failed: {e}\")\n",
    "                cv2.imwrite(frame_path, frame_bgr)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "    for frame_idx in range(total_frames):\n",
    "        img = cv2.imread(os.path.join(frames_dir, f\"frame_{frame_idx:05d}.png\"))\n",
    "        if img is not None:\n",
    "            writer.write(img)\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)      # original frames\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)   # deoldify video\n",
    "    cap_sam = cv2.VideoCapture(sam_path)          # sam masks\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion: base is ORIGINAL frame\n",
    "            fusion_frame = frame_in.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e9d443-4b24-41da-8769-2f077bffdffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3/3 [00:11<00:00,  3.69s/frame]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outputs written to outputs/Dr - Trim\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/Dr - Trim/Dr - Trim_yolo.mp4\n",
      " - deoldify: outputs/Dr - Trim/Dr - Trim_deoldify.mp4\n",
      " - sam: outputs/Dr - Trim/Dr - Trim_sam.mp4\n",
      " - final: outputs/Dr - Trim/Dr - Trim_final.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # your ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None):\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    if to_path is None:\n",
    "        to_path = filename\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return to_path\n",
    "\n",
    "def run_sam_on_frame(frame_rgb, comfy_server=COMFY):\n",
    "    tmp_path = f\"temp_frame_{uuid.uuid4().hex}.png\"\n",
    "    Image.fromarray(frame_rgb).save(tmp_path)\n",
    "    uploaded = upload_image_to_comfy(tmp_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON,\"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_loadimage_node(prompt, uploaded)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time()+60\n",
    "    seg_path = None\n",
    "    while time.time()<deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    seg_path = download_image(im[\"filename\"], server=comfy_server,\n",
    "                                              subfolder=im.get(\"subfolder\",\"\"),\n",
    "                                              folder_type=im.get(\"type\",\"output\"))\n",
    "                    break\n",
    "        if seg_path: break\n",
    "        time.sleep(0.5)\n",
    "    os.remove(tmp_path)\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Output paths\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer_yolo = cv2.VideoWriter(yolo_path, fourcc, fps, (width,height))\n",
    "    writer_deoldify = cv2.VideoWriter(deoldify_path, fourcc, fps, (width,height))\n",
    "    writer_sam = cv2.VideoWriter(sam_path, fourcc, fps, (width,height))\n",
    "    writer_fusion = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"Processing\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # --- YOLO detections ---\n",
    "            results = yolo_model.predict(frame_bgr, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer_yolo.write(results[0].plot())\n",
    "\n",
    "            # --- DeOldify full-frame ---\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer_deoldify.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # --- SAM segmentation ---\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                if seg_img is None:\n",
    "                    seg_resized = frame_bgr\n",
    "                else:\n",
    "                    seg_resized = cv2.resize(seg_img, (width,height))\n",
    "                writer_sam.write(seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ SAM failed on frame: {e}\")\n",
    "                seg_resized = frame_bgr\n",
    "                writer_sam.write(frame_bgr)\n",
    "\n",
    "            # --- Fusion: apply DeOldify only on SAM ∩ YOLO ---\n",
    "            fusion_frame = frame_rgb.copy()\n",
    "            if results and results[0].boxes is not None:\n",
    "                boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "                confs = results[0].boxes.conf.cpu().numpy()\n",
    "                classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "                gray = cv2.cvtColor(seg_resized, cv2.COLOR_BGR2GRAY)\n",
    "                _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                for box, conf, cls in zip(boxes, confs, classes):\n",
    "                    if int(cls) != 0 or conf < CONF_THRESHOLD:\n",
    "                        continue  # only person class with conf >= 0.6\n",
    "\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "                    yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "                    intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "                    mask_bool = intersect > 127\n",
    "\n",
    "                    fusion_frame[mask_bool] = deold[mask_bool]\n",
    "\n",
    "            writer_fusion.write(cv2.cvtColor(fusion_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer_yolo.release()\n",
    "    writer_deoldify.release()\n",
    "    writer_sam.release()\n",
    "    writer_fusion.release()\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Example call ---\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/Dr - Trim.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k,v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d64533-9e6c-42df-bceb-dac90fe57a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "\n",
    "\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6c1135-7dd9-4a60-845c-2cb1eef08f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/deoldify/fastai/data_block.py:451: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\n",
      "  warn(\"Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.\")\n",
      "/opt/deoldify/fastai/data_block.py:453: UserWarning: Your validation set is empty. If this is by design, use `split_none()`\n",
      "                 or pass `ignore_empty=True` when labelling to remove this warning.\n",
      "  warn(\"\"\"Your validation set is empty. If this is by design, use `split_none()`\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/dl_env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/opt/deoldify/fastai/basic_train.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(tmp_file)\n",
      "/opt/deoldify/fastai/basic_train.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(source, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None):\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    if to_path is None:\n",
    "        to_path = filename\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return to_path\n",
    "\n",
    "def run_sam_on_frame(frame_rgb, comfy_server=COMFY):\n",
    "    tmp_path = f\"temp_frame_{uuid.uuid4().hex}.png\"\n",
    "    Image.fromarray(frame_rgb).save(tmp_path)\n",
    "    uploaded = upload_image_to_comfy(tmp_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON,\"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_loadimage_node(prompt, uploaded)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time()+60\n",
    "    seg_path = None\n",
    "    while time.time()<deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    seg_path = download_image(im[\"filename\"], server=comfy_server,\n",
    "                                              subfolder=im.get(\"subfolder\",\"\"),\n",
    "                                              folder_type=im.get(\"type\",\"output\"))\n",
    "                    break\n",
    "        if seg_path: break\n",
    "        time.sleep(0.5)\n",
    "    os.remove(tmp_path)\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width,height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            results_per_frame.append(results[0])\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                seg_resized = cv2.resize(seg_img, (width,height)) if seg_img is not None else frame_bgr\n",
    "                writer.write(seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ SAM failed: {e}\")\n",
    "                writer.write(frame_bgr)\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # reload DeOldify\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "\n",
    "            # reload SAM\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                gray = cv2.cvtColor(seg_img, cv2.COLOR_BGR2GRAY)\n",
    "                _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "            except:\n",
    "                sam_mask = np.zeros((height,width), dtype=np.uint8)\n",
    "\n",
    "            # YOLO boxes\n",
    "            fusion_frame = frame_rgb.copy()\n",
    "            if frame_idx < len(yolo_results):\n",
    "                boxes = yolo_results[frame_idx].boxes.xyxy.cpu().numpy()\n",
    "                confs = yolo_results[frame_idx].boxes.conf.cpu().numpy()\n",
    "                classes = yolo_results[frame_idx].boxes.cls.cpu().numpy()\n",
    "                for box, conf, cls in zip(boxes, confs, classes):\n",
    "                    if int(cls) != 0 or conf < CONF_THRESHOLD:\n",
    "                        continue\n",
    "                    x1,y1,x2,y2 = map(int, box)\n",
    "                    yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "                    yolo_mask[y1:y2,x1:x2] = 255\n",
    "                    intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "                    mask_bool = intersect > 127\n",
    "                    fusion_frame[mask_bool] = deold[mask_bool]\n",
    "\n",
    "            writer.write(cv2.cvtColor(fusion_frame, cv2.COLOR_RGB2BGR))\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# # ---- Example ----\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_video = \"input_videos/THATHA MANAVADU colored Trim.mp4\"\n",
    "#     outputs = process_video(input_video)\n",
    "#     print(\"Pipeline outputs:\")\n",
    "#     for k,v in outputs.items():\n",
    "#         print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928f6e6c-0b0f-4dbf-887b-3cb7d3ba04c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4baa01ab-4765-41b1-bab6-7996bf83523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n",
      "[CACHE] Using cached YOLO: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_yolo.mp4\n",
      "[CACHE] Using cached DeOldify: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_deoldify.mp4\n",
      "[CACHE] Using cached SAM: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_sam.mp4\n",
      "[CACHE] Using cached Fusion: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_final.mp4\n",
      "[INFO] Outputs written to outputs/THATHA MANAVADU colored Trim\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_yolo.mp4\n",
      " - deoldify: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_deoldify.mp4\n",
      " - sam: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_sam.mp4\n",
      " - final: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_final.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    if os.path.exists(yolo_path):\n",
    "        print(f\"[CACHE] Using cached YOLO: {yolo_path}\")\n",
    "        return yolo_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return yolo_path\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                seg_resized = cv2.resize(seg_img, (width, height)) if seg_img is not None else frame_bgr\n",
    "                writer.write(seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ SAM failed: {e}\")\n",
    "                writer.write(frame_bgr)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(yolo_path, deoldify_path, sam_path, out_dir, name):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_yolo = cv2.VideoCapture(yolo_path)\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_yolo.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_yolo.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_yolo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        cap_yolo.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        for _ in range(total_frames):\n",
    "            ret1, frame_yolo = cap_yolo.read()\n",
    "            ret2, frame_deold = cap_deold.read()\n",
    "            ret3, frame_sam = cap_sam.read()\n",
    "            if not (ret1 and ret2 and ret3):\n",
    "                break\n",
    "\n",
    "            # Build mask from SAM video\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "            mask_bool = sam_mask > 127\n",
    "\n",
    "            # Fusion: base is YOLO frame, apply DeOldify where mask is valid\n",
    "            fusion_frame = frame_yolo.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_yolo.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_fusion(input_path, yolo_results, deoldify_path, sam_path, out_dir, name, conf_thresh=0.6):\n",
    "    \"\"\"\n",
    "    Fusion: Apply DeOldify color only where SAM mask ∩ YOLO(person, conf>=0.6) overlap.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): original input video path (for reference size/fps)\n",
    "        yolo_results (list): list of YOLO results per frame (from run_yolo)\n",
    "        deoldify_path (str): path to DeOldify output video\n",
    "        sam_path (str): path to SAM output video\n",
    "        out_dir (str): output directory\n",
    "        name (str): base name\n",
    "        conf_thresh (float): YOLO confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        str: path to fusion video\n",
    "    \"\"\"\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    # Open video sources\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_deold.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_deold.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_deold.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # --- Build SAM mask ---\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # --- Build YOLO mask ---\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            boxes = yolo_results[frame_idx].boxes.xyxy.cpu().numpy()\n",
    "            confs = yolo_results[frame_idx].boxes.conf.cpu().numpy()\n",
    "            classes = yolo_results[frame_idx].boxes.cls.cpu().numpy()\n",
    "\n",
    "            for box, conf, cls in zip(boxes, confs, classes):\n",
    "                if int(cls) != 0 or conf < conf_thresh:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # --- Intersection mask (SAM ∩ YOLO) ---\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # --- Apply DeOldify only where intersection is True ---\n",
    "            fusion_frame = frame_deold.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, yolo_path, deoldify_path, sam_path, out_dir, name)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/THATHA MANAVADU colored Trim.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26b8a4f-b1e5-48f1-9ba9-e521f1113107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Using: NVIDIA GeForce RTX 4060 Ti\n",
      "[INFO] Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO: 100%|██████████| 1871/1871 [00:47<00:00, 39.15frame/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CACHE] Using cached DeOldify: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_deoldify.mp4\n",
      "[CACHE] Using cached SAM: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_sam.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusion: 100%|██████████| 1871/1871 [00:24<00:00, 75.69frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fusion video saved: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_final.mp4\n",
      "[INFO] Outputs written to outputs/THATHA MANAVADU colored Trim\n",
      "Pipeline outputs:\n",
      " - yolo: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_yolo.mp4\n",
      " - deoldify: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_deoldify.mp4\n",
      " - sam: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_sam.mp4\n",
      " - final: outputs/THATHA MANAVADU colored Trim/THATHA MANAVADU colored Trim_final.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "\n",
    "# yolo_model = YOLO(YOLO_MODEL_PATH).to(\"cuda\")\n",
    "# print(f\"YOLO running on: {yolo_model.device}\")\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            # Store only necessary info to reduce memory\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Here, you should integrate your SAM inference instead of dummy mask\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            seg_resized = frame_bgr  # fallback\n",
    "            writer.write(seg_resized)\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(yolo_results, deoldify_path, sam_path, out_dir, name):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)\n",
    "    cap_sam = cv2.VideoCapture(sam_path)\n",
    "\n",
    "    fps = int(cap_deold.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_deold.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_deold.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion: apply DeOldify where intersection\n",
    "            fusion_frame = frame_deold.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(yolo_results, deoldify_path, sam_path, out_dir, name)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/THATHA MANAVADU colored Trim.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73751eb-8ddc-4d0d-be91-a2044e559815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Using: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9734336a-c83d-441d-88e4-fbc0ed4bc41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[CACHE] Using cached YOLO + results: outputs/thatha_manavadu_test/thatha_manavadu_test_yolo.mp4\n",
      "[CACHE] Using cached DeOldify: outputs/thatha_manavadu_test/thatha_manavadu_test_deoldify.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAM:  15%|█▍        | 20/137 [00:49<04:56,  2.54s/frame]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SAM failed: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAM:  60%|█████▉    | 82/137 [03:26<02:18,  2.52s/frame]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 405\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    404\u001b[39m     input_video = \u001b[33m\"\u001b[39m\u001b[33minput_videos/thatha_manavadu_test.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     outputs = \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_video\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPipeline outputs:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 390\u001b[39m, in \u001b[36mprocess_video\u001b[39m\u001b[34m(input_path)\u001b[39m\n\u001b[32m    388\u001b[39m yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n\u001b[32m    389\u001b[39m deoldify_path = run_deoldify(input_path, out_dir, name)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m sam_path = \u001b[43mrun_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n\u001b[32m    393\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INFO] Outputs written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mrun_sam\u001b[39m\u001b[34m(input_path, out_dir, name)\u001b[39m\n\u001b[32m    220\u001b[39m frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     seg_path = \u001b[43mrun_sam_on_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     seg_img = cv2.imread(seg_path)\n\u001b[32m    224\u001b[39m     seg_resized = cv2.resize(seg_img, (width, height)) \u001b[38;5;28;01mif\u001b[39;00m seg_img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m frame_bgr\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mrun_sam_on_frame\u001b[39m\u001b[34m(frame_rgb, comfy_server)\u001b[39m\n\u001b[32m     99\u001b[39m tmp_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtemp_frame_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid.uuid4().hex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m Image.fromarray(frame_rgb).save(tmp_path)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m uploaded = \u001b[43mupload_image_to_comfy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomfy_server\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(WORKFLOW_JSON, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    104\u001b[39m     prompt = json.load(f)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mupload_image_to_comfy\u001b[39m\u001b[34m(local_path, server, dest_name, folder_type)\u001b[39m\n\u001b[32m     44\u001b[39m     files = {\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: (dest_name, f, \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m)}\n\u001b[32m     45\u001b[39m     data = {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: folder_type, \u001b[33m\"\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mserver\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/upload/image\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     r.raise_for_status()\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dest_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/http/client.py:1374\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1373\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1376\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/http/client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/http/client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/dl_env/lib/python3.11/socket.py:705\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    707\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from deoldify.visualize import get_image_colorizer\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from PIL import Image\n",
    "import uuid, json, requests, time\n",
    "\n",
    "# ==== CONFIG ====\n",
    "YOLO_MODEL_PATH = \"models/yolo11x-seg.pt\"\n",
    "CONF_THRESHOLD = 0.6\n",
    "OUTPUT_ROOT = \"outputs\"\n",
    "COMFY = \"http://192.168.27.13:23476\"    # ComfyUI server\n",
    "WORKFLOW_JSON = \"ClothesDetect_api.json\"\n",
    "# =================\n",
    "\n",
    "# ---- Setup DeOldify ----\n",
    "device.set(device=DeviceId.GPU0)\n",
    "colorizer = get_image_colorizer(artistic=True)\n",
    "\n",
    "# ---- Setup YOLO ----\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device_str}\")\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH).to(device_str)\n",
    "\n",
    "\n",
    "# ---- DeOldify inference ----\n",
    "def deoldify_inference(frame_rgb):\n",
    "    pil_img = Image.fromarray(frame_rgb).convert(\"RGB\")\n",
    "    ret = colorizer.get_transformed_image(pil_img, render_factor=16, post_process=True)\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# ---- ComfyUI helpers ----\n",
    "def upload_image_to_comfy(local_path, server=COMFY, *, dest_name=None, folder_type=\"input\"):\n",
    "    if dest_name is None:\n",
    "        dest_name = os.path.basename(local_path)\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        files = {\"image\": (dest_name, f, \"image/png\")}\n",
    "        data = {\"type\": folder_type, \"overwrite\": \"true\"}\n",
    "        r = requests.post(f\"{server}/upload/image\", files=files, data=data, timeout=60)\n",
    "        r.raise_for_status()\n",
    "    return dest_name\n",
    "\n",
    "def patch_loadimage_node(prompt_dict, new_filename):\n",
    "    for node in prompt_dict.values():\n",
    "        if node.get(\"class_type\",\"\").lower() == \"loadimage\":\n",
    "            node[\"inputs\"][\"image\"] = new_filename\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def queue_prompt(prompt_dict, server=COMFY):\n",
    "    client_id = str(uuid.uuid4())\n",
    "    r = requests.post(f\"{server}/prompt\", json={\"prompt\": prompt_dict, \"client_id\": client_id}, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"prompt_id\", client_id)\n",
    "\n",
    "def get_history(prompt_id, server=COMFY):\n",
    "    r = requests.get(f\"{server}/history/{prompt_id}\", timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None):\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    if to_path is None:\n",
    "        to_path = filename\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return to_path\n",
    "\n",
    "\n",
    "def download_image(filename, server=COMFY, folder_type=\"output\", subfolder=\"\", to_path=None, save_dir=None):\n",
    "    # If not specified, save under OUTPUT_ROOT/comfy_downloads/\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, \"comfy_downloads\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    r = requests.get(f\"{server}/view\", params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if to_path is None:\n",
    "        to_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(to_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    return to_path\n",
    "\n",
    "\n",
    "def run_sam_on_frame(frame_rgb, comfy_server=COMFY):\n",
    "    tmp_path = f\"temp_frame_{uuid.uuid4().hex}.png\"\n",
    "    Image.fromarray(frame_rgb).save(tmp_path)\n",
    "    uploaded = upload_image_to_comfy(tmp_path, server=comfy_server)\n",
    "\n",
    "    with open(WORKFLOW_JSON, \"r\") as f:\n",
    "        prompt = json.load(f)\n",
    "    patch_loadimage_node(prompt, uploaded)\n",
    "\n",
    "    prompt_id = queue_prompt(prompt, server=comfy_server)\n",
    "    deadline = time.time() + 60\n",
    "    seg_path = None\n",
    "    while time.time() < deadline:\n",
    "        hist = get_history(prompt_id, server=comfy_server)\n",
    "        item = hist.get(prompt_id)\n",
    "        if item and \"outputs\" in item:\n",
    "            for node_out in item[\"outputs\"].values():\n",
    "                for im in node_out.get(\"images\", []):\n",
    "                    seg_path = download_image(\n",
    "                        im[\"filename\"], server=comfy_server,\n",
    "                        subfolder=im.get(\"subfolder\", \"\"),\n",
    "                        folder_type=im.get(\"type\", \"output\")\n",
    "                    )\n",
    "                    break\n",
    "        if seg_path: break\n",
    "        time.sleep(0.5)\n",
    "    os.remove(tmp_path)\n",
    "    return seg_path\n",
    "\n",
    "\n",
    "# ---- Stage 1: YOLO ----\n",
    "def run_yolo(input_path, out_dir, name):\n",
    "    yolo_path = os.path.join(out_dir, f\"{name}_yolo.mp4\")\n",
    "    results_path = os.path.join(out_dir, f\"{name}_yolo_results.pkl\")\n",
    "\n",
    "    if os.path.exists(yolo_path) and os.path.exists(results_path):\n",
    "        print(f\"[CACHE] Using cached YOLO + results: {yolo_path}\")\n",
    "        with open(results_path, \"rb\") as f:\n",
    "            results_per_frame = pickle.load(f)\n",
    "        return yolo_path, results_per_frame\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(yolo_path, fourcc, fps, (width, height))\n",
    "\n",
    "    results_per_frame = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"YOLO\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            results = yolo_model.predict(frame, conf=CONF_THRESHOLD, verbose=False, device=device_str)\n",
    "            writer.write(results[0].plot())\n",
    "            results_per_frame.append({\n",
    "                \"boxes\": results[0].boxes.xyxy.cpu().numpy(),\n",
    "                \"conf\": results[0].boxes.conf.cpu().numpy(),\n",
    "                \"cls\": results[0].boxes.cls.cpu().numpy()\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results_per_frame, f)\n",
    "\n",
    "    return yolo_path, results_per_frame\n",
    "\n",
    "\n",
    "# ---- Stage 2: DeOldify ----\n",
    "def run_deoldify(input_path, out_dir, name):\n",
    "    deoldify_path = os.path.join(out_dir, f\"{name}_deoldify.mp4\")\n",
    "    if os.path.exists(deoldify_path):\n",
    "        print(f\"[CACHE] Using cached DeOldify: {deoldify_path}\")\n",
    "        return deoldify_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(deoldify_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"DeOldify\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            deold = deoldify_inference(frame_rgb)\n",
    "            writer.write(cv2.cvtColor(deold, cv2.COLOR_RGB2BGR))\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return deoldify_path\n",
    "\n",
    "\n",
    "# ---- Stage 3: SAM ----\n",
    "def run_sam(input_path, out_dir, name):\n",
    "    sam_path = os.path.join(out_dir, f\"{name}_sam.mp4\")\n",
    "    if os.path.exists(sam_path):\n",
    "        print(f\"[CACHE] Using cached SAM: {sam_path}\")\n",
    "        return sam_path\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(sam_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"SAM\", unit=\"frame\") as pbar:\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                seg_path = run_sam_on_frame(frame_rgb)\n",
    "                seg_img = cv2.imread(seg_path)\n",
    "                seg_resized = cv2.resize(seg_img, (width, height)) if seg_img is not None else frame_bgr\n",
    "                writer.write(seg_resized)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ SAM failed: {e}\")\n",
    "                writer.write(frame_bgr)\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    return sam_path\n",
    "\n",
    "\n",
    "# ---- Stage 4: Fusion ----\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)       # original\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)    # colorized\n",
    "    cap_sam = cv2.VideoCapture(sam_path)           # masks\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width, height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        len(yolo_results)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_bgr = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # original gray\n",
    "            frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "            frame_original = cv2.cvtColor(frame_gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion\n",
    "            fusion_frame = frame_original.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path):\n",
    "    fusion_path = os.path.join(out_dir, f\"{name}_final.mp4\")\n",
    "    if os.path.exists(fusion_path):\n",
    "        print(f\"[CACHE] Using cached Fusion: {fusion_path}\")\n",
    "        return fusion_path\n",
    "\n",
    "    cap_input = cv2.VideoCapture(input_path)      # original frames\n",
    "    cap_deold = cv2.VideoCapture(deoldify_path)   # deoldify video\n",
    "    cap_sam = cv2.VideoCapture(sam_path)          # sam masks\n",
    "\n",
    "    fps = int(cap_input.get(cv2.CAP_PROP_FPS)) or 25\n",
    "    width = int(cap_input.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap_input.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(fusion_path, fourcc, fps, (width,height))\n",
    "\n",
    "    total_frames = int(min(\n",
    "        len(yolo_results),\n",
    "        cap_input.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_deold.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        cap_sam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    ))\n",
    "\n",
    "    frame_idx = 0\n",
    "    with tqdm(total=total_frames, desc=\"Fusion\", unit=\"frame\") as pbar:\n",
    "        while frame_idx < total_frames:\n",
    "            ret_in, frame_in = cap_input.read()\n",
    "            ret_deold, frame_deold = cap_deold.read()\n",
    "            ret_sam, frame_sam = cap_sam.read()\n",
    "            if not (ret_in and ret_deold and ret_sam):\n",
    "                break\n",
    "\n",
    "            # SAM mask\n",
    "            gray = cv2.cvtColor(frame_sam, cv2.COLOR_BGR2GRAY)\n",
    "            _, sam_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # YOLO mask\n",
    "            yolo_mask = np.zeros_like(sam_mask, dtype=np.uint8)\n",
    "            for box, conf, cls in zip(\n",
    "                yolo_results[frame_idx][\"boxes\"],\n",
    "                yolo_results[frame_idx][\"conf\"],\n",
    "                yolo_results[frame_idx][\"cls\"]\n",
    "            ):\n",
    "                if int(cls) != 0 or conf < CONF_THRESHOLD:  # only \"person\"\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                yolo_mask[y1:y2, x1:x2] = 255\n",
    "\n",
    "            # Intersection\n",
    "            intersect = cv2.bitwise_and(sam_mask, yolo_mask)\n",
    "            mask_bool = intersect > 127\n",
    "\n",
    "            # Fusion: base is ORIGINAL frame\n",
    "            fusion_frame = frame_in.copy()\n",
    "            fusion_frame[mask_bool] = frame_deold[mask_bool]\n",
    "\n",
    "            writer.write(fusion_frame)\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap_input.release()\n",
    "    cap_deold.release()\n",
    "    cap_sam.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"[INFO] Fusion video saved: {fusion_path}\")\n",
    "    return fusion_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Main Pipeline ----\n",
    "def process_video(input_path):\n",
    "    folder, fname = os.path.split(input_path)\n",
    "    name, _ = os.path.splitext(fname)\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    yolo_path, yolo_results = run_yolo(input_path, out_dir, name)\n",
    "    deoldify_path = run_deoldify(input_path, out_dir, name)\n",
    "    sam_path = run_sam(input_path, out_dir, name)\n",
    "    fusion_path = run_fusion(input_path, out_dir, name, yolo_results, deoldify_path, sam_path)\n",
    "\n",
    "    print(f\"[INFO] Outputs written to {out_dir}\")\n",
    "    return {\n",
    "        \"yolo\": yolo_path,\n",
    "        \"deoldify\": deoldify_path,\n",
    "        \"sam\": sam_path,\n",
    "        \"final\": fusion_path\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Example ----\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"input_videos/thatha_manavadu_test.mp4\"\n",
    "    outputs = process_video(input_video)\n",
    "    print(\"Pipeline outputs:\")\n",
    "    for k, v in outputs.items():\n",
    "        print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c52772-179c-482d-9ac4-67d132d0bada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
